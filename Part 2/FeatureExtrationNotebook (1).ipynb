{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#This is an example of performing Video Activity Recognition using LSTM\n",
    "Modified from \"Hands-on Computer Vision with TensorFlow 2\" by B. Planche and E. Andres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: opencv-python in c:\\users\\stsc.lnvo-126908.000\\anaconda3\\lib\\site-packages (4.1.2.30)\n",
      "Requirement already satisfied: numpy>=1.14.5 in c:\\users\\stsc.lnvo-126908.000\\appdata\\roaming\\python\\python37\\site-packages (from opencv-python) (1.17.4)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\stsc.lnvo-126908.000\\anaconda3\\lib\\site-packages (3.1.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\stsc.lnvo-126908.000\\anaconda3\\lib\\site-packages (from matplotlib) (0.10.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\stsc.lnvo-126908.000\\anaconda3\\lib\\site-packages (from matplotlib) (1.1.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in c:\\users\\stsc.lnvo-126908.000\\anaconda3\\lib\\site-packages (from matplotlib) (2.4.2)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in c:\\users\\stsc.lnvo-126908.000\\anaconda3\\lib\\site-packages (from matplotlib) (2.8.0)\n",
      "Requirement already satisfied: numpy>=1.11 in c:\\users\\stsc.lnvo-126908.000\\appdata\\roaming\\python\\python37\\site-packages (from matplotlib) (1.17.4)\n",
      "Requirement already satisfied: six in c:\\users\\stsc.lnvo-126908.000\\anaconda3\\lib\\site-packages (from cycler>=0.10->matplotlib) (1.12.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\stsc.lnvo-126908.000\\anaconda3\\lib\\site-packages (from kiwisolver>=1.0.1->matplotlib) (41.4.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\stsc.lnvo-126908.000\\anaconda3\\lib\\site-packages (4.36.1)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\stsc.lnvo-126908.000\\anaconda3\\lib\\site-packages (0.21.3)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\stsc.lnvo-126908.000\\anaconda3\\lib\\site-packages (from scikit-learn) (0.13.2)\n",
      "Requirement already satisfied: numpy>=1.11.0 in c:\\users\\stsc.lnvo-126908.000\\appdata\\roaming\\python\\python37\\site-packages (from scikit-learn) (1.17.4)\n",
      "Requirement already satisfied: scipy>=0.17.0 in c:\\users\\stsc.lnvo-126908.000\\anaconda3\\lib\\site-packages (from scikit-learn) (1.3.1)\n"
     ]
    }
   ],
   "source": [
    "# STEP 1:  Install packages in the current environment\n",
    "import sys\n",
    "!{sys.executable} -m pip install opencv-python\n",
    "!{sys.executable} -m pip install matplotlib\n",
    "!{sys.executable} -m pip install tqdm\n",
    "!{sys.executable} -m pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow==2.0 in c:\\users\\stsc.lnvo-126908.000\\anaconda3\\lib\\site-packages (2.0.0)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in c:\\users\\stsc.lnvo-126908.000\\anaconda3\\lib\\site-packages (from tensorflow==2.0) (1.25.0)\n",
      "Requirement already satisfied: wheel>=0.26 in c:\\users\\stsc.lnvo-126908.000\\anaconda3\\lib\\site-packages (from tensorflow==2.0) (0.33.6)\n",
      "Requirement already satisfied: numpy<2.0,>=1.16.0 in c:\\users\\stsc.lnvo-126908.000\\appdata\\roaming\\python\\python37\\site-packages (from tensorflow==2.0) (1.17.4)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in c:\\users\\stsc.lnvo-126908.000\\anaconda3\\lib\\site-packages (from tensorflow==2.0) (1.1.0)\n",
      "Requirement already satisfied: astor>=0.6.0 in c:\\users\\stsc.lnvo-126908.000\\anaconda3\\lib\\site-packages (from tensorflow==2.0) (0.8.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\stsc.lnvo-126908.000\\anaconda3\\lib\\site-packages (from tensorflow==2.0) (3.1.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\stsc.lnvo-126908.000\\anaconda3\\lib\\site-packages (from tensorflow==2.0) (1.1.0)\n",
      "Requirement already satisfied: protobuf>=3.6.1 in c:\\users\\stsc.lnvo-126908.000\\anaconda3\\lib\\site-packages (from tensorflow==2.0) (3.11.0)\n",
      "Requirement already satisfied: wrapt>=1.11.1 in c:\\users\\stsc.lnvo-126908.000\\anaconda3\\lib\\site-packages (from tensorflow==2.0) (1.11.2)\n",
      "Requirement already satisfied: google-pasta>=0.1.6 in c:\\users\\stsc.lnvo-126908.000\\anaconda3\\lib\\site-packages (from tensorflow==2.0) (0.1.8)\n",
      "Requirement already satisfied: gast==0.2.2 in c:\\users\\stsc.lnvo-126908.000\\anaconda3\\lib\\site-packages (from tensorflow==2.0) (0.2.2)\n",
      "Requirement already satisfied: keras-applications>=1.0.8 in c:\\users\\stsc.lnvo-126908.000\\anaconda3\\lib\\site-packages (from tensorflow==2.0) (1.0.8)\n",
      "Requirement already satisfied: absl-py>=0.7.0 in c:\\users\\stsc.lnvo-126908.000\\anaconda3\\lib\\site-packages (from tensorflow==2.0) (0.8.1)\n",
      "Requirement already satisfied: tensorboard<2.1.0,>=2.0.0 in c:\\users\\stsc.lnvo-126908.000\\anaconda3\\lib\\site-packages (from tensorflow==2.0) (2.0.2)\n",
      "Requirement already satisfied: tensorflow-estimator<2.1.0,>=2.0.0 in c:\\users\\stsc.lnvo-126908.000\\anaconda3\\lib\\site-packages (from tensorflow==2.0) (2.0.1)\n",
      "Requirement already satisfied: six>=1.10.0 in c:\\users\\stsc.lnvo-126908.000\\anaconda3\\lib\\site-packages (from tensorflow==2.0) (1.12.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\stsc.lnvo-126908.000\\anaconda3\\lib\\site-packages (from protobuf>=3.6.1->tensorflow==2.0) (41.4.0)\n",
      "Requirement already satisfied: h5py in c:\\users\\stsc.lnvo-126908.000\\anaconda3\\lib\\site-packages (from keras-applications>=1.0.8->tensorflow==2.0) (2.9.0)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\stsc.lnvo-126908.000\\anaconda3\\lib\\site-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow==2.0) (2.22.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in c:\\users\\stsc.lnvo-126908.000\\anaconda3\\lib\\site-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow==2.0) (0.4.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\stsc.lnvo-126908.000\\anaconda3\\lib\\site-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow==2.0) (3.1.1)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in c:\\users\\stsc.lnvo-126908.000\\anaconda3\\lib\\site-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow==2.0) (1.7.1)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in c:\\users\\stsc.lnvo-126908.000\\anaconda3\\lib\\site-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow==2.0) (0.16.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\stsc.lnvo-126908.000\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0) (2019.9.11)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in c:\\users\\stsc.lnvo-126908.000\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\stsc.lnvo-126908.000\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0) (1.24.2)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in c:\\users\\stsc.lnvo-126908.000\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0) (2.8)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\stsc.lnvo-126908.000\\anaconda3\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0) (1.3.0)\n",
      "Requirement already satisfied: rsa<4.1,>=3.1.4 in c:\\users\\stsc.lnvo-126908.000\\anaconda3\\lib\\site-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0) (4.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\stsc.lnvo-126908.000\\anaconda3\\lib\\site-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0) (0.2.7)\n",
      "Requirement already satisfied: cachetools<3.2,>=2.0.0 in c:\\users\\stsc.lnvo-126908.000\\anaconda3\\lib\\site-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0) (3.1.1)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\stsc.lnvo-126908.000\\anaconda3\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0) (3.1.0)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in c:\\users\\stsc.lnvo-126908.000\\anaconda3\\lib\\site-packages (from rsa<4.1,>=3.1.4->google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0) (0.4.8)\n"
     ]
    }
   ],
   "source": [
    "#!{sys.executable} -m pip install tensorflow==1.14\n",
    "!{sys.executable} -m pip install tensorflow==2.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Step 2: import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\STSC.LNVO-126908.000\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3326, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-10-79673bd9355c>\", line 1, in <module>\n",
      "    import tensorflow as tf\n",
      "  File \"C:\\Users\\STSC.LNVO-126908.000\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\__init__.py\", line 98, in <module>\n",
      "    from tensorflow_core import *\n",
      "  File \"C:\\Users\\STSC.LNVO-126908.000\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\__init__.py\", line 42, in <module>\n",
      "    from . _api.v2 import audio\n",
      "ModuleNotFoundError: No module named 'tensorflow_core._api.v2'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\STSC.LNVO-126908.000\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2040, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'ModuleNotFoundError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\STSC.LNVO-126908.000\\Anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1101, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"C:\\Users\\STSC.LNVO-126908.000\\Anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 319, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Users\\STSC.LNVO-126908.000\\Anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 353, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"C:\\Users\\STSC.LNVO-126908.000\\Anaconda3\\lib\\inspect.py\", line 1502, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"C:\\Users\\STSC.LNVO-126908.000\\Anaconda3\\lib\\inspect.py\", line 1460, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"C:\\Users\\STSC.LNVO-126908.000\\Anaconda3\\lib\\inspect.py\", line 696, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"C:\\Users\\STSC.LNVO-126908.000\\Anaconda3\\lib\\inspect.py\", line 733, in getmodule\n",
      "    if ismodule(module) and hasattr(module, '__file__'):\n",
      "  File \"C:\\Users\\STSC.LNVO-126908.000\\Anaconda3\\lib\\site-packages\\tensorflow\\__init__.py\", line 50, in __getattr__\n",
      "  File \"C:\\Users\\STSC.LNVO-126908.000\\Anaconda3\\lib\\site-packages\\tensorflow\\__init__.py\", line 44, in _load\n",
      "  File \"C:\\Users\\STSC.LNVO-126908.000\\Anaconda3\\lib\\importlib\\__init__.py\", line 127, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"<frozen importlib._bootstrap>\", line 1006, in _gcd_import\n",
      "  File \"<frozen importlib._bootstrap>\", line 983, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 953, in _find_and_load_unlocked\n",
      "  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\n",
      "  File \"<frozen importlib._bootstrap>\", line 1006, in _gcd_import\n",
      "  File \"<frozen importlib._bootstrap>\", line 983, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 967, in _find_and_load_unlocked\n",
      "  File \"<frozen importlib._bootstrap>\", line 677, in _load_unlocked\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 728, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\n",
      "  File \"C:\\Users\\STSC.LNVO-126908.000\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\__init__.py\", line 42, in <module>\n",
      "    from . _api.v2 import audio\n",
      "ModuleNotFoundError: No module named 'tensorflow_core._api.v2'\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow_core._api.v2'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tqdm\n",
    "from sklearn.preprocessing import LabelBinarizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf.enable_eager_execution()\n",
    "#tf.executing_eagerly()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Step 3: setup variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\STSC.LNVO-126908.000\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3326, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-11-b7ec6b68b2fc>\", line 7, in <module>\n",
      "    VIDEOS_PATH = os.path.join(BASE_PATH, '**','*.mp4')\n",
      "NameError: name 'os' is not defined\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\STSC.LNVO-126908.000\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2040, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'NameError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\STSC.LNVO-126908.000\\Anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1101, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"C:\\Users\\STSC.LNVO-126908.000\\Anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 319, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Users\\STSC.LNVO-126908.000\\Anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 353, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"C:\\Users\\STSC.LNVO-126908.000\\Anaconda3\\lib\\inspect.py\", line 1502, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"C:\\Users\\STSC.LNVO-126908.000\\Anaconda3\\lib\\inspect.py\", line 1460, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"C:\\Users\\STSC.LNVO-126908.000\\Anaconda3\\lib\\inspect.py\", line 696, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"C:\\Users\\STSC.LNVO-126908.000\\Anaconda3\\lib\\inspect.py\", line 733, in getmodule\n",
      "    if ismodule(module) and hasattr(module, '__file__'):\n",
      "  File \"C:\\Users\\STSC.LNVO-126908.000\\Anaconda3\\lib\\site-packages\\tensorflow\\__init__.py\", line 50, in __getattr__\n",
      "  File \"C:\\Users\\STSC.LNVO-126908.000\\Anaconda3\\lib\\site-packages\\tensorflow\\__init__.py\", line 44, in _load\n",
      "  File \"C:\\Users\\STSC.LNVO-126908.000\\Anaconda3\\lib\\importlib\\__init__.py\", line 127, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"<frozen importlib._bootstrap>\", line 1006, in _gcd_import\n",
      "  File \"<frozen importlib._bootstrap>\", line 983, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 953, in _find_and_load_unlocked\n",
      "  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\n",
      "  File \"<frozen importlib._bootstrap>\", line 1006, in _gcd_import\n",
      "  File \"<frozen importlib._bootstrap>\", line 983, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 967, in _find_and_load_unlocked\n",
      "  File \"<frozen importlib._bootstrap>\", line 677, in _load_unlocked\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 728, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\n",
      "  File \"C:\\Users\\STSC.LNVO-126908.000\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\__init__.py\", line 42, in <module>\n",
      "    from . _api.v2 import audio\n",
      "ModuleNotFoundError: No module named 'tensorflow_core._api.v2'\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m"
     ]
    }
   ],
   "source": [
    "#location of where YOU have installed the data set UCF-101 located\n",
    "# at \n",
    "#BASE_PATH = '../data/UCF-101'\n",
    "#change the base path to location YOU installed UCF-101 dataset\n",
    "#BASE_PATH = 'C:/Grewe/Classes/CS663/Mat/LSTM/data/UCF-101'\n",
    "BASE_PATH = 'C:\\\\Users\\\\STSC.LNVO-126908.000\\\\Desktop\\\\FallDetection\\\\Classes\\\\'\n",
    "VIDEOS_PATH = os.path.join(BASE_PATH, '**','*.mp4')\n",
    "\n",
    "#this specifies the sequence length will process by LSTM\n",
    "SEQUENCE_LENGTH = 40\n",
    "BATCH_SIZE = 16\n",
    "print(VIDEOS_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 4:  sample the video --do not process every frame\n",
    "PART 1: define function frame_generator() that creates Sequence_length samples by taking every Kth sample were K= num_frames_in_video / SEQUENCE LENGTH     PART 2: you load the DataSet and specify the output will be frames of size 299x299 x3(rgb) AND you create batches of 16 together at a time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def frame_generator():\n",
    "    video_paths = tf.io.gfile.glob(VIDEOS_PATH)\n",
    "    np.random.shuffle(video_paths)\n",
    "    for video_path in video_paths:\n",
    "        frames = []\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        num_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        sample_every_frame = max(1, num_frames // SEQUENCE_LENGTH)\n",
    "        current_frame = 0\n",
    "        \n",
    "        label = os.path.basename(os.path.dirname(video_path))\n",
    "        \n",
    "        max_images = SEQUENCE_LENGTH\n",
    "        while True:\n",
    "            success, frame = cap.read()\n",
    "            if not success:\n",
    "                break\n",
    "                \n",
    "            if current_frame % sample_every_frame == 0:\n",
    "                frame = frame[:, :, ::-1]\n",
    "                img = tf.image.resize(frame, (224, 224))\n",
    "                img = tf.keras.applications.mobilenet_v2.preprocess_input(img)\n",
    "                max_images -= 1\n",
    "                yield img, video_path\n",
    "                \n",
    "            if max_images == 0:\n",
    "                break\n",
    "            current_frame += 1\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_generator(frame_generator,\n",
    "                                         output_types=(tf.float32, tf.string),\n",
    "                                         output_shapes=((224, 224, 3), ()))\n",
    "\n",
    "dataset = dataset.batch(BATCH_SIZE).prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<DatasetV1Adapter shapes: ((None, 224, 224, 3), (None,)), types: (tf.float32, tf.string)>\n"
     ]
    }
   ],
   "source": [
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 5: \n",
    "For Feature Extraction we are going to use a existing CNN model called Inception V3 which is built into TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mobilenet_v2 = tf.keras.applications.mobilenet_v2.MobileNetV2(input_shape=(224,224,3), include_top=False, weights='imagenet')\n",
    "x = mobilenet_v2.output\n",
    "\n",
    "# We add Average Pooling to transform the feature map from\n",
    "# 8 * 8 * 2048 to 1 x 2048, as we don't need spatial information\n",
    "pooling_output = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
    "feature_extraction_model = tf.keras.Model(mobilenet_v2.input,pooling_output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 6: \n",
    "Extract Features using our InceptionV3 CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1308it [20:24,  1.07it/s]\n"
     ]
    }
   ],
   "source": [
    "#Don't run this for if done with feature extraction \n",
    "current_path = None\n",
    "all_features = []\n",
    "\n",
    "#cycle through the dataset and visit each image, note the tdqm is a progress bar\n",
    "#that updates each time a new iteration is called \n",
    "#call feature_extraction_model above (Inception v3) for the image to extract the features\n",
    "for img, batch_paths in tqdm.tqdm(dataset):\n",
    "    batch_features = feature_extraction_model(img)\n",
    "    #reshape the tensor \n",
    "    batch_features = tf.reshape(batch_features, \n",
    "                              (batch_features.shape[0], -1))\n",
    "    \n",
    "    for features, path in zip(batch_features.numpy(), batch_paths.numpy()):\n",
    "        if path != current_path and current_path is not None:\n",
    "            output_path = current_path.decode().replace('.mp4', '.npy')\n",
    "            np.save(output_path, all_features)\n",
    "            all_features = []\n",
    "            \n",
    "        current_path = path\n",
    "        all_features.append(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 7: \n",
    "Create a MyLabelBinarizer for 2 Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run this\n",
    "\n",
    "class MyLabelBinarizer(LabelBinarizer):\n",
    "    def transform(self, y):\n",
    "        Y = super().transform(y)\n",
    "        if self.y_type_ == 'binary':\n",
    "            return np.hstack((Y, 1-Y))\n",
    "        else:\n",
    "            return Y\n",
    "    def inverse_transform(self, Y, threshold=None):\n",
    "        if self.y_type_ == 'binary':\n",
    "            return super().inverse_transform(Y[:, 0], threshold)\n",
    "        else:\n",
    "            return super().inverse_transform(Y, threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Falling' 'Walking']\n",
      "[[0 1]\n",
      " [1 0]]\n",
      "[[0 1]\n",
      " [1 0]\n",
      " [1 0]]\n",
      "['Falling' 'Walking' 'Walking']\n",
      "length of labrels 2\n"
     ]
    }
   ],
   "source": [
    "#run this\n",
    "LABELS = ['Falling','Walking']\n",
    "encoder = MyLabelBinarizer()\n",
    "encoder.fit(LABELS)\n",
    "print(encoder.classes_)\n",
    "print(encoder.transform(['Falling', 'Walking']))\n",
    "\n",
    "t= encoder.transform(['Falling', 'Walking', 'Walking'])\n",
    "print(t)\n",
    "print(encoder.inverse_transform(t))\n",
    "print(\"length of labrels \" + str(len(LABELS)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 8: \n",
    "    Create the LSTM model:    1) Masking layer  2) LSTM layer with 512 cells, dropout 0.5, recurrent_dropout of 0.5  \n",
    " 3) a fully connected relu activation layer with 256 outputs,  4) a droupout layer 0.5  5) a final decision fully connected layer of putput length of labels  (which is the number of classes) with softmax activation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setup a keras Sequential model with 1) Masking layer  2) LSTM layer with 512 cells, dropout 0.5, recurrent_dropout of 0.5  \n",
    "# 3) a fully connected relu activation layer with 256 outputs,  4) a droupout layer 5) a final decision fully connected layer of length of labels\n",
    "# (which is the number of classes) with softmax activation.\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Masking(mask_value=0.),\n",
    "    tf.keras.layers.LSTM(512, dropout=0.5, recurrent_dropout=0.5),\n",
    "    tf.keras.layers.Dense(256, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(2, activation='softmax')\n",
    "    #tf.keras.layers.Dense(len(LABELS), activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 8: \n",
    "Setup for the model the Loss function, the Optimizer function, and any metrics want to compute in training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy', 'top_k_categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 9: \n",
    "Setup  the training and test list which are lists of the training filenames.   Note you will need to change the location of these files to point to your location.  Define a function make_generator that returns a generator which will randomly shuffle a file list (either training or testing that will be passed later) and then changes the file extension of the avi files listed in the list to .npy which is our features for that avi video which were calcluated in step 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_file = '/Users/subhi/Downloads/CV-proj3/Data/trainlist.txt'\n",
    "#test_file = '/Users/subhi/Downloads/CV-proj3/Data/testlist.txt'\n",
    "test_file = 'C:/Users/STSC.LNVO-126908.000/Desktop/FallDetection/trainlist.txt'\n",
    "train_file = 'C:/Users/STSC.LNVO-126908.000/Desktop/FallDetection/testlist.txt'\n",
    "\n",
    "with open(test_file) as f:\n",
    "    test_list = [row.strip() for row in list(f)]\n",
    "\n",
    "with open(train_file) as f:\n",
    "     train_list = [row.strip() for row in list(f)]\n",
    "    #train_list = [row.split(' ')[0] for row in train_list]\n",
    "#print(train_list)\n",
    "def make_generator(file_list):\n",
    "    def generator():\n",
    "        np.random.shuffle(file_list)\n",
    "        for path in file_list:\n",
    "            full_path = os.path.join(BASE_PATH, path).replace('.mp4', '.npy')\n",
    "            \n",
    "            label = os.path.basename(os.path.dirname(path))\n",
    "            features = np.load(full_path)\n",
    "            \n",
    "            \n",
    "            padded_sequence = np.zeros((SEQUENCE_LENGTH, 1280))\n",
    "            padded_sequence[0:len(features)] = np.array(features)\n",
    "            \n",
    "            transformed_label = encoder.transform([label])\n",
    "            \n",
    "            yield padded_sequence, transformed_label[0]\n",
    "    return generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Falling\\\\Falling_0.mp4', 'Falling\\\\Falling_1.mp4', 'Falling\\\\Falling_2.mp4', 'Falling\\\\Falling_3.mp4', 'Falling\\\\Falling_4.mp4', 'Falling\\\\Falling_5.mp4', 'Falling\\\\Falling_6.mp4', 'Falling\\\\Falling_7.mp4', 'Falling\\\\Falling_30.mp4', 'Falling\\\\Falling_31.mp4', 'Falling\\\\Falling_32.mp4', 'Falling\\\\Falling_33.mp4', 'Falling\\\\Falling_34.mp4', 'Falling\\\\Falling_35.mp4', 'Falling\\\\Falling_36.mp4', 'Falling\\\\Falling_37.mp4', 'Falling\\\\Falling_38.mp4', 'Falling\\\\Falling_39.mp4', 'Falling\\\\Falling_40.mp4', 'Falling\\\\Falling_41.mp4', 'Falling\\\\Falling_42.mp4', 'Falling\\\\Falling_43.mp4', 'Falling\\\\Falling_44.mp4', 'Falling\\\\Falling_45.mp4', 'Falling\\\\Falling_46.mp4', 'Falling\\\\Falling_47.mp4', 'Falling\\\\Falling_48.mp4', 'Falling\\\\Falling_49.mp4', 'Falling\\\\Falling_50.mp4', 'Falling\\\\Falling_51.mp4', 'Falling\\\\Falling_52.mp4', 'Falling\\\\Falling_53.mp4', 'Falling\\\\Falling_54.mp4', 'Falling\\\\Falling_55.mp4', 'Falling\\\\Falling_56.mp4', 'Falling\\\\Falling_57.mp4', 'Falling\\\\Falling_58.mp4', 'Falling\\\\Falling_59.mp4', 'Falling\\\\Falling_60.mp4', 'Falling\\\\Falling_61.mp4', 'Falling\\\\Falling_62.mp4', 'Falling\\\\Falling_63.mp4', 'Falling\\\\Falling_64.mp4', 'Falling\\\\Falling_65.mp4', 'Falling\\\\Falling_66.mp4', 'Falling\\\\Falling_67.mp4', 'Falling\\\\Falling_68.mp4', 'Falling\\\\Falling_69.mp4', 'Falling\\\\Falling_70.mp4', 'Falling\\\\Falling_71.mp4', 'Falling\\\\Falling_72.mp4', 'Falling\\\\Falling_73.mp4', 'Falling\\\\Falling_74.mp4', 'Falling\\\\Falling_75.mp4', 'Falling\\\\Falling_76.mp4', 'Falling\\\\Falling_77.mp4', 'Falling\\\\Falling_78.mp4', 'Falling\\\\Falling_79.mp4', 'Falling\\\\Falling_80.mp4', 'Falling\\\\Falling_81.mp4', 'Falling\\\\Falling_82.mp4', 'Falling\\\\Falling_83.mp4', 'Falling\\\\Falling_84.mp4', 'Falling\\\\Falling_85.mp4', 'Falling\\\\Falling_104.mp4', 'Falling\\\\Falling_105.mp4', 'Falling\\\\Falling_129.mp4', 'Falling\\\\Falling_130.mp4', 'Falling\\\\Falling_131.mp4', 'Falling\\\\Falling_132.mp4', 'Falling\\\\Falling_157.mp4', 'Falling\\\\Falling_158.mp4', 'Falling\\\\Falling_159.mp4', 'Falling\\\\Falling_160.mp4', 'Falling\\\\Falling_161.mp4', 'Falling\\\\Falling_162.mp4', 'Falling\\\\Falling_163.mp4', 'Falling\\\\Falling_164.mp4', 'Falling\\\\Falling_165.mp4', 'Falling\\\\Falling_166.mp4', 'Falling\\\\Falling_167.mp4', 'Falling\\\\Falling_168.mp4', 'Falling\\\\Falling_169.mp4', 'Falling\\\\Falling_170.mp4', 'Falling\\\\Falling_171.mp4', 'Falling\\\\Falling_172.mp4', 'Falling\\\\Falling_173.mp4', 'Falling\\\\Falling_174.mp4', 'Falling\\\\Falling_175.mp4', 'Falling\\\\Falling_176.mp4', 'Falling\\\\Falling_177.mp4', 'Falling\\\\Falling_178.mp4', 'Falling\\\\Falling_179.mp4', 'Falling\\\\Falling_180.mp4', 'Falling\\\\Falling_181.mp4', 'Falling\\\\Falling_182.mp4', 'Falling\\\\Falling_183.mp4', 'Falling\\\\Falling_184.mp4', 'Falling\\\\Falling_185.mp4', 'Falling\\\\Falling_186.mp4', 'Falling\\\\Falling_187.mp4', 'Falling\\\\Falling_188.mp4', 'Falling\\\\Falling_189.mp4', 'Falling\\\\Falling_190.mp4', 'Falling\\\\Falling_191.mp4', 'Falling\\\\Falling_192.mp4', 'Falling\\\\Falling_193.mp4', 'Falling\\\\Falling_194.mp4', 'Falling\\\\Falling_195.mp4', 'Falling\\\\Falling_196.mp4', 'Falling\\\\Falling_197.mp4', 'Falling\\\\Falling_198.mp4', 'Falling\\\\Falling_199.mp4', 'Falling\\\\Falling_200.mp4', 'Falling\\\\Falling_250.mp4', 'Falling\\\\Falling_251.mp4', 'Falling\\\\Falling_252.mp4', 'Falling\\\\Falling_253.mp4', 'Falling\\\\Falling_254.mp4', 'Falling\\\\Falling_255.mp4', 'Falling\\\\Falling_256.mp4', 'Falling\\\\Falling_257.mp4', 'Falling\\\\Falling_258.mp4', 'Falling\\\\Falling_259.mp4', 'Falling\\\\Falling_260.mp4', 'Falling\\\\Falling_261.mp4', 'Falling\\\\Falling_262.mp4', 'Falling\\\\Falling_263.mp4', 'Falling\\\\Falling_264.mp4', 'Falling\\\\Falling_265.mp4', 'Falling\\\\Falling_266.mp4', 'Falling\\\\Falling_267.mp4', 'Falling\\\\Falling_268.mp4', 'Falling\\\\Falling_269.mp4', 'Falling\\\\Falling_270.mp4', 'Falling\\\\Falling_271.mp4', 'Falling\\\\Falling_272.mp4', 'Falling\\\\Falling_273.mp4', 'Falling\\\\Falling_274.mp4', 'Falling\\\\Falling_275.mp4', 'Falling\\\\Falling_276.mp4', 'Falling\\\\Falling_277.mp4', 'Falling\\\\Falling_278.mp4', 'Falling\\\\Falling_279.mp4', 'Falling\\\\Falling_280.mp4', 'Falling\\\\Falling_281.mp4', 'Falling\\\\Falling_282.mp4', 'Falling\\\\Falling_283.mp4', 'Falling\\\\Falling_284.mp4', 'Falling\\\\Falling_285.mp4', 'Falling\\\\Falling_286.mp4', 'Falling\\\\Falling_287.mp4', 'Falling\\\\Falling_310.mp4', 'Falling\\\\Falling_311.mp4', 'Falling\\\\Falling_345.mp4', 'Falling\\\\Falling_346.mp4', 'Falling\\\\Falling_347.mp4', 'Falling\\\\Falling_348.mp4', 'Falling\\\\Falling_349.mp4', 'Falling\\\\Falling_350.mp4', 'Falling\\\\Falling_351.mp4', 'Falling\\\\Falling_352.mp4', 'Falling\\\\Falling_353.mp4', 'Falling\\\\Falling_354.mp4', 'Falling\\\\Falling_355.mp4', 'Falling\\\\Falling_401.mp4', 'Falling\\\\Falling_402.mp4', 'Falling\\\\Falling_403.mp4', 'Falling\\\\Falling_404.mp4', 'Falling\\\\Falling_405.mp4', 'Falling\\\\Falling_406.mp4', 'Falling\\\\Falling_407.mp4', 'Falling\\\\Falling_408.mp4', 'Falling\\\\Falling_409.mp4', 'Falling\\\\Falling_410.mp4', 'Falling\\\\Falling_411.mp4', 'Falling\\\\Falling_412.mp4', 'Falling\\\\Falling_413.mp4', 'Falling\\\\Falling_414.mp4', 'Falling\\\\Falling_415.mp4', 'Falling\\\\Falling_416.mp4', 'Falling\\\\Falling_417.mp4', 'Falling\\\\Falling_418.mp4', 'Falling\\\\Falling_419.mp4', 'Falling\\\\Falling_420.mp4', 'Falling\\\\Falling_445.mp4', 'Falling\\\\Falling_446.mp4', 'Falling\\\\Falling_447.mp4', 'Falling\\\\Falling_448.mp4', 'Falling\\\\Falling_449.mp4', 'Falling\\\\Falling_450.mp4', 'Falling\\\\Falling_451.mp4', 'Falling\\\\Falling_452.mp4', 'Falling\\\\Falling_453.mp4', 'Falling\\\\Falling_454.mp4', 'Falling\\\\Falling_455.mp4', 'Falling\\\\Falling_456.mp4', 'Falling\\\\Falling_457.mp4', 'Falling\\\\Falling_458.mp4', 'Falling\\\\Falling_459.mp4', 'Falling\\\\Falling_460.mp4', 'Falling\\\\Falling_461.mp4', 'Falling\\\\Falling_462.mp4', 'Falling\\\\Falling_463.mp4', 'Falling\\\\Falling_464.mp4', 'Falling\\\\Falling_465.mp4', 'Falling\\\\Falling_466.mp4', 'Falling\\\\Falling_467.mp4', 'Falling\\\\Falling_468.mp4', 'Falling\\\\Falling_469.mp4', 'Falling\\\\Falling_470.mp4', 'Falling\\\\Falling_471.mp4', 'Falling\\\\Falling_472.mp4', 'Falling\\\\Falling_473.mp4', 'Falling\\\\Falling_474.mp4', 'Falling\\\\Falling_475.mp4', 'Falling\\\\Falling_476.mp4', 'Falling\\\\Falling_477.mp4', 'Falling\\\\Falling_478.mp4', 'Falling\\\\Falling_479.mp4', 'Falling\\\\Falling_480.mp4', 'Falling\\\\Falling_481.mp4', 'Falling\\\\Falling_482.mp4', 'Falling\\\\Falling_483.mp4', 'Falling\\\\Falling_484.mp4', 'Falling\\\\Falling_485.mp4', 'Falling\\\\Falling_486.mp4', 'Falling\\\\Falling_487.mp4', 'Falling\\\\Falling_488.mp4', 'Falling\\\\Falling_489.mp4', 'Falling\\\\Falling_490.mp4', 'Falling\\\\Falling_491.mp4', 'Falling\\\\Falling_492.mp4', 'Falling\\\\Falling_493.mp4', 'Falling\\\\Falling_494.mp4', 'Falling\\\\Falling_495.mp4', 'Falling\\\\Falling_496.mp4', 'Falling\\\\Falling_497.mp4', 'Falling\\\\Falling_498.mp4', 'Falling\\\\Falling_499.mp4', 'Falling\\\\Falling_500.mp4', 'Falling\\\\Falling_542.mp4', 'Falling\\\\Falling_543.mp4', 'Falling\\\\Falling_544.mp4', 'Falling\\\\Falling_545.mp4', 'Falling\\\\Falling_546.mp4', 'Falling\\\\Falling_547.mp4', 'Falling\\\\Falling_548.mp4', 'Falling\\\\Falling_549.mp4', 'Falling\\\\Falling_550.mp4', 'Falling\\\\Falling_551.mp4', 'Falling\\\\Falling_552.mp4', 'Falling\\\\Falling_553.mp4', 'Falling\\\\Falling_554.mp4', 'Falling\\\\Falling_555.mp4', 'Falling\\\\Falling_580.mp4', 'Falling\\\\Falling_581.mp4', 'Falling\\\\Falling_582.mp4', 'Falling\\\\Falling_583.mp4', 'Falling\\\\Falling_584.mp4', 'Falling\\\\Falling_585.mp4', 'Walking\\\\Walking0.mp4', 'Walking\\\\Walking1.mp4', 'Walking\\\\Walking2.mp4', 'Walking\\\\Walking3.mp4', 'Walking\\\\Walking4.mp4', 'Walking\\\\Walking5.mp4', 'Walking\\\\Walking6.mp4', 'Walking\\\\Walking7.mp4', 'Walking\\\\Walking8.mp4', 'Walking\\\\Walking9.mp4', 'Walking\\\\Walking10.mp4', 'Walking\\\\Walking11.mp4', 'Walking\\\\Walking30.mp4', 'Walking\\\\Walking31.mp4', 'Walking\\\\Walking32.mp4', 'Walking\\\\Walking33.mp4', 'Walking\\\\Walking34.mp4', 'Walking\\\\Walking35.mp4', 'Walking\\\\Walking36.mp4', 'Walking\\\\Walking62.mp4', 'Walking\\\\Walking63.mp4', 'Walking\\\\Walking64.mp4', 'Walking\\\\Walking65.mp4', 'Walking\\\\Walking85.mp4', 'Walking\\\\Walking86.mp4', 'Walking\\\\Walking87.mp4', 'Walking\\\\Walking88.mp4', 'Walking\\\\Walking89.mp4', 'Walking\\\\Walking90.mp4', 'Walking\\\\Walking91.mp4', 'Walking\\\\Walking92.mp4', 'Walking\\\\Walking93.mp4', 'Walking\\\\Walking100.mp4', 'Walking\\\\Walking101.mp4', 'Walking\\\\Walking102.mp4', 'Walking\\\\Walking103.mp4', 'Walking\\\\Walking104.mp4', 'Walking\\\\Walking105.mp4', 'Walking\\\\Walking106.mp4', 'Walking\\\\Walking107.mp4', 'Walking\\\\Walking108.mp4', 'Walking\\\\Walking109.mp4', 'Walking\\\\Walking110.mp4', 'Walking\\\\Walking111.mp4', 'Walking\\\\Walking112.mp4', 'Walking\\\\Walking113.mp4', 'Walking\\\\Walking114.mp4', 'Walking\\\\Walking157.mp4', 'Walking\\\\Walking158.mp4', 'Walking\\\\Walking159.mp4', 'Walking\\\\Walking160.mp4', 'Walking\\\\Walking161.mp4', 'Walking\\\\Walking162.mp4', 'Walking\\\\Walking163.mp4', 'Walking\\\\Walking164.mp4', 'Walking\\\\Walking165.mp4', 'Walking\\\\Walking166.mp4', 'Walking\\\\Walking167.mp4', 'Walking\\\\Walking168.mp4', 'Walking\\\\Walking169.mp4', 'Walking\\\\Walking170.mp4', 'Walking\\\\Walking171.mp4', 'Walking\\\\Walking172.mp4', 'Walking\\\\Walking173.mp4', 'Walking\\\\Walking174.mp4', 'Walking\\\\Walking175.mp4', 'Walking\\\\Walking176.mp4', 'Walking\\\\Walking177.mp4', 'Walking\\\\Walking178.mp4', 'Walking\\\\Walking179.mp4', 'Walking\\\\Walking180.mp4', 'Walking\\\\Walking181.mp4', 'Walking\\\\Walking182.mp4', 'Walking\\\\Walking183.mp4', 'Walking\\\\Walking184.mp4', 'Walking\\\\Walking185.mp4', 'Walking\\\\Walking186.mp4', 'Walking\\\\Walking187.mp4', 'Walking\\\\Walking188.mp4', 'Walking\\\\Walking233.mp4', 'Walking\\\\Walking234.mp4', 'Walking\\\\Walking235.mp4', 'Walking\\\\Walking236.mp4', 'Walking\\\\Walking237.mp4', 'Walking\\\\Walking238.mp4', 'Walking\\\\Walking239.mp4', 'Walking\\\\Walking240.mp4', 'Walking\\\\Walking241.mp4', 'Walking\\\\Walking242.mp4', 'Walking\\\\Walking243.mp4', 'Walking\\\\Walking244.mp4', 'Walking\\\\Walking245.mp4', 'Walking\\\\Walking246.mp4', 'Walking\\\\Walking247.mp4', 'Walking\\\\Walking255.mp4', 'Walking\\\\Walking256.mp4', 'Walking\\\\Walking257.mp4', 'Walking\\\\Walking258.mp4', 'Walking\\\\Walking276.mp4', 'Walking\\\\Walking277.mp4', 'Walking\\\\Walking278.mp4', 'Walking\\\\Walking279.mp4', 'Walking\\\\Walking280.mp4', 'Walking\\\\Walking281.mp4', 'Walking\\\\Walking282.mp4', 'Walking\\\\Walking283.mp4', 'Walking\\\\Walking286.mp4', 'Walking\\\\Walking287.mp4', 'Walking\\\\Walking288.mp4', 'Walking\\\\Walking289.mp4', 'Walking\\\\Walking290.mp4', 'Walking\\\\Walking310.mp4', 'Walking\\\\Walking311.mp4', 'Walking\\\\Walking312.mp4', 'Walking\\\\Walking313.mp4', 'Walking\\\\Walking314.mp4', 'Walking\\\\Walking315.mp4', 'Walking\\\\Walking316.mp4', 'Walking\\\\Walking317.mp4', 'Walking\\\\Walking318.mp4', 'Walking\\\\Walking337.mp4', 'Walking\\\\Walking338.mp4', 'Walking\\\\Walking339.mp4', 'Walking\\\\Walking340.mp4', 'Walking\\\\Walking341.mp4', 'Walking\\\\Walking342.mp4', 'Walking\\\\Walking343.mp4', 'Walking\\\\Walking344.mp4', 'Walking\\\\Walking345.mp4', 'Walking\\\\Walking346.mp4', 'Walking\\\\Walking347.mp4', 'Walking\\\\Walking348.mp4', 'Walking\\\\Walking349.mp4', 'Walking\\\\Walking350.mp4', 'Walking\\\\Walking372.mp4', 'Walking\\\\Walking373.mp4', 'Walking\\\\Walking374.mp4', 'Walking\\\\Walking375.mp4', 'Walking\\\\Walking376.mp4', 'Walking\\\\Walking377.mp4', 'Walking\\\\Walking378.mp4', 'Walking\\\\Walking379.mp4', 'Walking\\\\Walking380.mp4', 'Walking\\\\Walking381.mp4', 'Walking\\\\Walking382.mp4', 'Walking\\\\Walking383.mp4', 'Walking\\\\Walking384.mp4', 'Walking\\\\Walking385.mp4', 'Walking\\\\Walking408.mp4', 'Walking\\\\Walking409.mp4', 'Walking\\\\Walking410.mp4', 'Walking\\\\Walking411.mp4', 'Walking\\\\Walking412.mp4', 'Walking\\\\Walking413.mp4', 'Walking\\\\Walking414.mp4', 'Walking\\\\Walking415.mp4', 'Walking\\\\Walking416.mp4', 'Walking\\\\Walking417.mp4', 'Walking\\\\Walking418.mp4', 'Walking\\\\Walking440.mp4', 'Walking\\\\Walking441.mp4', 'Walking\\\\Walking442.mp4', 'Walking\\\\Walking443.mp4', 'Walking\\\\Walking444.mp4', 'Walking\\\\Walking445.mp4', 'Walking\\\\Walking446.mp4', 'Walking\\\\Walking447.mp4', 'Walking\\\\Walking448.mp4', 'Walking\\\\Walking468.mp4', 'Walking\\\\Walking469.mp4', 'Walking\\\\Walking470.mp4', 'Walking\\\\Walking471.mp4', 'Walking\\\\Walking472.mp4', 'Walking\\\\Walking473.mp4', 'Walking\\\\Walking474.mp4', 'Walking\\\\Walking475.mp4', 'Walking\\\\Walking476.mp4', 'Walking\\\\Walking477.mp4', 'Walking\\\\Walking478.mp4', 'Walking\\\\Walking479.mp4', 'Walking\\\\Walking480.mp4', 'Walking\\\\Walking481.mp4', 'Walking\\\\Walking482.mp4', 'Walking\\\\Walking520.mp4', 'Walking\\\\Walking521.mp4', 'Walking\\\\Walking522.mp4', 'Walking\\\\Walking523.mp4', 'Walking\\\\Walking524.mp4', 'Walking\\\\Walking525.mp4', 'Walking\\\\Walking526.mp4', 'Walking\\\\Walking527.mp4', 'Walking\\\\Walking538.mp4', 'Walking\\\\Walking539.mp4', 'Walking\\\\Walking540.mp4', 'Walking\\\\Walking541.mp4', 'Walking\\\\Walking542.mp4', 'Walking\\\\Walking543.mp4', 'Walking\\\\Walking544.mp4', 'Walking\\\\Walking545.mp4', 'Walking\\\\Walking546.mp4', 'Walking\\\\Walking547.mp4', 'Walking\\\\Walking548.mp4', 'Walking\\\\Walking549.mp4', 'Walking\\\\Walking550.mp4', 'Walking\\\\Walking551.mp4', 'Walking\\\\Walking568.mp4', 'Walking\\\\Walking569.mp4', 'Walking\\\\Walking571.mp4', 'Walking\\\\Walking572.mp4', 'Walking\\\\Walking573.mp4', 'Walking\\\\Walking574.mp4', 'Walking\\\\Walking575.mp4', 'Walking\\\\Walking576.mp4', 'Walking\\\\Walking577.mp4', 'Walking\\\\Walking578.mp4', 'Walking\\\\Walking579.mp4', 'Walking\\\\Walking580.mp4', 'Walking\\\\Walking581.mp4', 'Walking\\\\Walking582.mp4', 'Walking\\\\Walking589.mp4', 'Walking\\\\Walking590.mp4', 'Walking\\\\Walking591.mp4', 'Walking\\\\Walking592.mp4', 'Walking\\\\Walking611.mp4', 'Walking\\\\Walking612.mp4', 'Walking\\\\Walking613.mp4', 'Walking\\\\Walking614.mp4', 'Walking\\\\Walking615.mp4', 'Walking\\\\Walking616.mp4', 'Walking\\\\Walking617.mp4', 'Walking\\\\Walking618.mp4', 'Walking\\\\Walking619.mp4', 'Walking\\\\Walking620.mp4', 'Walking\\\\Walking621.mp4', 'Walking\\\\Walking622.mp4']\n"
     ]
    }
   ],
   "source": [
    "print(train_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Falling\\\\Falling_0.mp4', 'Falling\\\\Falling_1.mp4', 'Falling\\\\Falling_2.mp4', 'Falling\\\\Falling_3.mp4', 'Falling\\\\Falling_4.mp4', 'Falling\\\\Falling_5.mp4', 'Falling\\\\Falling_6.mp4', 'Falling\\\\Falling_7.mp4', 'Falling\\\\Falling_8.mp4', 'Falling\\\\Falling_9.mp4', 'Falling\\\\Falling_10.mp4', 'Falling\\\\Falling_11.mp4', 'Falling\\\\Falling_12.mp4', 'Falling\\\\Falling_13.mp4', 'Falling\\\\Falling_14.mp4', 'Falling\\\\Falling_15.mp4', 'Falling\\\\Falling_16.mp4', 'Falling\\\\Falling_17.mp4', 'Falling\\\\Falling_18.mp4', 'Falling\\\\Falling_19.mp4', 'Falling\\\\Falling_20.mp4', 'Falling\\\\Falling_21.mp4', 'Falling\\\\Falling_22.mp4', 'Falling\\\\Falling_23.mp4', 'Falling\\\\Falling_24.mp4', 'Falling\\\\Falling_25.mp4', 'Falling\\\\Falling_26.mp4', 'Falling\\\\Falling_27.mp4', 'Falling\\\\Falling_28.mp4', 'Falling\\\\Falling_29.mp4', 'Falling\\\\Falling_30.mp4', 'Falling\\\\Falling_31.mp4', 'Falling\\\\Falling_32.mp4', 'Falling\\\\Falling_33.mp4', 'Falling\\\\Falling_34.mp4', 'Falling\\\\Falling_35.mp4', 'Falling\\\\Falling_36.mp4', 'Falling\\\\Falling_37.mp4', 'Falling\\\\Falling_38.mp4', 'Falling\\\\Falling_39.mp4', 'Falling\\\\Falling_40.mp4', 'Falling\\\\Falling_41.mp4', 'Falling\\\\Falling_42.mp4', 'Falling\\\\Falling_43.mp4', 'Falling\\\\Falling_44.mp4', 'Falling\\\\Falling_45.mp4', 'Falling\\\\Falling_46.mp4', 'Falling\\\\Falling_47.mp4', 'Falling\\\\Falling_48.mp4', 'Falling\\\\Falling_49.mp4', 'Falling\\\\Falling_50.mp4', 'Falling\\\\Falling_51.mp4', 'Falling\\\\Falling_52.mp4', 'Falling\\\\Falling_53.mp4', 'Falling\\\\Falling_54.mp4', 'Falling\\\\Falling_55.mp4', 'Falling\\\\Falling_56.mp4', 'Falling\\\\Falling_57.mp4', 'Falling\\\\Falling_58.mp4', 'Falling\\\\Falling_59.mp4', 'Falling\\\\Falling_60.mp4', 'Falling\\\\Falling_61.mp4', 'Falling\\\\Falling_62.mp4', 'Falling\\\\Falling_63.mp4', 'Falling\\\\Falling_64.mp4', 'Falling\\\\Falling_65.mp4', 'Falling\\\\Falling_66.mp4', 'Falling\\\\Falling_67.mp4', 'Falling\\\\Falling_68.mp4', 'Falling\\\\Falling_69.mp4', 'Falling\\\\Falling_70.mp4', 'Falling\\\\Falling_71.mp4', 'Falling\\\\Falling_72.mp4', 'Falling\\\\Falling_73.mp4', 'Falling\\\\Falling_74.mp4', 'Falling\\\\Falling_75.mp4', 'Falling\\\\Falling_76.mp4', 'Falling\\\\Falling_77.mp4', 'Falling\\\\Falling_78.mp4', 'Falling\\\\Falling_79.mp4', 'Falling\\\\Falling_80.mp4', 'Falling\\\\Falling_81.mp4', 'Falling\\\\Falling_82.mp4', 'Falling\\\\Falling_83.mp4', 'Falling\\\\Falling_84.mp4', 'Falling\\\\Falling_85.mp4', 'Falling\\\\Falling_86.mp4', 'Falling\\\\Falling_87.mp4', 'Falling\\\\Falling_88.mp4', 'Falling\\\\Falling_89.mp4', 'Falling\\\\Falling_90.mp4', 'Falling\\\\Falling_91.mp4', 'Falling\\\\Falling_92.mp4', 'Falling\\\\Falling_93.mp4', 'Falling\\\\Falling_94.mp4', 'Falling\\\\Falling_95.mp4', 'Falling\\\\Falling_96.mp4', 'Falling\\\\Falling_97.mp4', 'Falling\\\\Falling_98.mp4', 'Falling\\\\Falling_99.mp4', 'Falling\\\\Falling_100.mp4', 'Falling\\\\Falling_101.mp4', 'Falling\\\\Falling_102.mp4', 'Falling\\\\Falling_103.mp4', 'Falling\\\\Falling_104.mp4', 'Falling\\\\Falling_105.mp4', 'Falling\\\\Falling_106.mp4', 'Falling\\\\Falling_107.mp4', 'Falling\\\\Falling_108.mp4', 'Falling\\\\Falling_109.mp4', 'Falling\\\\Falling_110.mp4', 'Falling\\\\Falling_111.mp4', 'Falling\\\\Falling_112.mp4', 'Falling\\\\Falling_113.mp4', 'Falling\\\\Falling_114.mp4', 'Falling\\\\Falling_115.mp4', 'Falling\\\\Falling_116.mp4', 'Falling\\\\Falling_117.mp4', 'Falling\\\\Falling_118.mp4', 'Falling\\\\Falling_119.mp4', 'Falling\\\\Falling_120.mp4', 'Falling\\\\Falling_121.mp4', 'Falling\\\\Falling_122.mp4', 'Falling\\\\Falling_123.mp4', 'Falling\\\\Falling_124.mp4', 'Falling\\\\Falling_125.mp4', 'Falling\\\\Falling_126.mp4', 'Falling\\\\Falling_127.mp4', 'Falling\\\\Falling_128.mp4', 'Falling\\\\Falling_129.mp4', 'Falling\\\\Falling_130.mp4', 'Falling\\\\Falling_131.mp4', 'Falling\\\\Falling_132.mp4', 'Falling\\\\Falling_133.mp4', 'Falling\\\\Falling_134.mp4', 'Falling\\\\Falling_135.mp4', 'Falling\\\\Falling_136.mp4', 'Falling\\\\Falling_137.mp4', 'Falling\\\\Falling_138.mp4', 'Falling\\\\Falling_139.mp4', 'Falling\\\\Falling_140.mp4', 'Falling\\\\Falling_141.mp4', 'Falling\\\\Falling_142.mp4', 'Falling\\\\Falling_143.mp4', 'Falling\\\\Falling_144.mp4', 'Falling\\\\Falling_145.mp4', 'Falling\\\\Falling_146.mp4', 'Falling\\\\Falling_147.mp4', 'Falling\\\\Falling_148.mp4', 'Falling\\\\Falling_149.mp4', 'Falling\\\\Falling_150.mp4', 'Falling\\\\Falling_151.mp4', 'Falling\\\\Falling_152.mp4', 'Falling\\\\Falling_153.mp4', 'Falling\\\\Falling_154.mp4', 'Falling\\\\Falling_155.mp4', 'Falling\\\\Falling_156.mp4', 'Falling\\\\Falling_157.mp4', 'Falling\\\\Falling_158.mp4', 'Falling\\\\Falling_159.mp4', 'Falling\\\\Falling_160.mp4', 'Falling\\\\Falling_161.mp4', 'Falling\\\\Falling_162.mp4', 'Falling\\\\Falling_163.mp4', 'Falling\\\\Falling_164.mp4', 'Falling\\\\Falling_165.mp4', 'Falling\\\\Falling_166.mp4', 'Falling\\\\Falling_167.mp4', 'Falling\\\\Falling_168.mp4', 'Falling\\\\Falling_169.mp4', 'Falling\\\\Falling_170.mp4', 'Falling\\\\Falling_171.mp4', 'Falling\\\\Falling_172.mp4', 'Falling\\\\Falling_173.mp4', 'Falling\\\\Falling_174.mp4', 'Falling\\\\Falling_175.mp4', 'Falling\\\\Falling_176.mp4', 'Falling\\\\Falling_177.mp4', 'Falling\\\\Falling_178.mp4', 'Falling\\\\Falling_179.mp4', 'Falling\\\\Falling_180.mp4', 'Falling\\\\Falling_181.mp4', 'Falling\\\\Falling_182.mp4', 'Falling\\\\Falling_183.mp4', 'Falling\\\\Falling_184.mp4', 'Falling\\\\Falling_185.mp4', 'Falling\\\\Falling_186.mp4', 'Falling\\\\Falling_187.mp4', 'Falling\\\\Falling_188.mp4', 'Falling\\\\Falling_189.mp4', 'Falling\\\\Falling_190.mp4', 'Falling\\\\Falling_191.mp4', 'Falling\\\\Falling_192.mp4', 'Falling\\\\Falling_193.mp4', 'Falling\\\\Falling_194.mp4', 'Falling\\\\Falling_195.mp4', 'Falling\\\\Falling_196.mp4', 'Falling\\\\Falling_197.mp4', 'Falling\\\\Falling_198.mp4', 'Falling\\\\Falling_199.mp4', 'Falling\\\\Falling_200.mp4', 'Falling\\\\Falling_201.mp4', 'Falling\\\\Falling_202.mp4', 'Falling\\\\Falling_203.mp4', 'Falling\\\\Falling_204.mp4', 'Falling\\\\Falling_205.mp4', 'Falling\\\\Falling_206.mp4', 'Falling\\\\Falling_207.mp4', 'Falling\\\\Falling_208.mp4', 'Falling\\\\Falling_209.mp4', 'Falling\\\\Falling_210.mp4', 'Falling\\\\Falling_211.mp4', 'Falling\\\\Falling_212.mp4', 'Falling\\\\Falling_213.mp4', 'Falling\\\\Falling_214.mp4', 'Falling\\\\Falling_215.mp4', 'Falling\\\\Falling_216.mp4', 'Falling\\\\Falling_217.mp4', 'Falling\\\\Falling_218.mp4', 'Falling\\\\Falling_219.mp4', 'Falling\\\\Falling_220.mp4', 'Falling\\\\Falling_221.mp4', 'Falling\\\\Falling_222.mp4', 'Falling\\\\Falling_223.mp4', 'Falling\\\\Falling_224.mp4', 'Falling\\\\Falling_225.mp4', 'Falling\\\\Falling_226.mp4', 'Falling\\\\Falling_227.mp4', 'Falling\\\\Falling_228.mp4', 'Falling\\\\Falling_229.mp4', 'Falling\\\\Falling_230.mp4', 'Falling\\\\Falling_231.mp4', 'Falling\\\\Falling_232.mp4', 'Falling\\\\Falling_233.mp4', 'Falling\\\\Falling_234.mp4', 'Falling\\\\Falling_235.mp4', 'Falling\\\\Falling_236.mp4', 'Falling\\\\Falling_237.mp4', 'Falling\\\\Falling_238.mp4', 'Falling\\\\Falling_239.mp4', 'Falling\\\\Falling_240.mp4', 'Falling\\\\Falling_241.mp4', 'Falling\\\\Falling_242.mp4', 'Falling\\\\Falling_243.mp4', 'Falling\\\\Falling_244.mp4', 'Falling\\\\Falling_245.mp4', 'Falling\\\\Falling_246.mp4', 'Falling\\\\Falling_247.mp4', 'Falling\\\\Falling_248.mp4', 'Falling\\\\Falling_249.mp4', 'Falling\\\\Falling_250.mp4', 'Falling\\\\Falling_251.mp4', 'Falling\\\\Falling_252.mp4', 'Falling\\\\Falling_253.mp4', 'Falling\\\\Falling_254.mp4', 'Falling\\\\Falling_255.mp4', 'Falling\\\\Falling_256.mp4', 'Falling\\\\Falling_257.mp4', 'Falling\\\\Falling_258.mp4', 'Falling\\\\Falling_259.mp4', 'Falling\\\\Falling_260.mp4', 'Falling\\\\Falling_261.mp4', 'Falling\\\\Falling_262.mp4', 'Falling\\\\Falling_263.mp4', 'Falling\\\\Falling_264.mp4', 'Falling\\\\Falling_265.mp4', 'Falling\\\\Falling_266.mp4', 'Falling\\\\Falling_267.mp4', 'Falling\\\\Falling_268.mp4', 'Falling\\\\Falling_269.mp4', 'Falling\\\\Falling_270.mp4', 'Falling\\\\Falling_271.mp4', 'Falling\\\\Falling_272.mp4', 'Falling\\\\Falling_273.mp4', 'Falling\\\\Falling_274.mp4', 'Falling\\\\Falling_275.mp4', 'Falling\\\\Falling_276.mp4', 'Falling\\\\Falling_277.mp4', 'Falling\\\\Falling_278.mp4', 'Falling\\\\Falling_279.mp4', 'Falling\\\\Falling_280.mp4', 'Falling\\\\Falling_281.mp4', 'Falling\\\\Falling_282.mp4', 'Falling\\\\Falling_283.mp4', 'Falling\\\\Falling_284.mp4', 'Falling\\\\Falling_285.mp4', 'Falling\\\\Falling_286.mp4', 'Falling\\\\Falling_287.mp4', 'Falling\\\\Falling_288.mp4', 'Falling\\\\Falling_289.mp4', 'Falling\\\\Falling_290.mp4', 'Falling\\\\Falling_291.mp4', 'Falling\\\\Falling_292.mp4', 'Falling\\\\Falling_293.mp4', 'Falling\\\\Falling_294.mp4', 'Falling\\\\Falling_295.mp4', 'Falling\\\\Falling_296.mp4', 'Falling\\\\Falling_297.mp4', 'Falling\\\\Falling_298.mp4', 'Falling\\\\Falling_299.mp4', 'Falling\\\\Falling_300.mp4', 'Falling\\\\Falling_301.mp4', 'Falling\\\\Falling_302.mp4', 'Falling\\\\Falling_303.mp4', 'Falling\\\\Falling_304.mp4', 'Falling\\\\Falling_305.mp4', 'Falling\\\\Falling_306.mp4', 'Falling\\\\Falling_307.mp4', 'Falling\\\\Falling_308.mp4', 'Falling\\\\Falling_309.mp4', 'Falling\\\\Falling_310.mp4', 'Falling\\\\Falling_311.mp4', 'Falling\\\\Falling_312.mp4', 'Falling\\\\Falling_313.mp4', 'Falling\\\\Falling_314.mp4', 'Falling\\\\Falling_315.mp4', 'Falling\\\\Falling_316.mp4', 'Falling\\\\Falling_317.mp4', 'Falling\\\\Falling_318.mp4', 'Falling\\\\Falling_319.mp4', 'Falling\\\\Falling_320.mp4', 'Falling\\\\Falling_321.mp4', 'Falling\\\\Falling_322.mp4', 'Falling\\\\Falling_323.mp4', 'Falling\\\\Falling_324.mp4', 'Falling\\\\Falling_325.mp4', 'Falling\\\\Falling_326.mp4', 'Falling\\\\Falling_327.mp4', 'Falling\\\\Falling_328.mp4', 'Falling\\\\Falling_329.mp4', 'Falling\\\\Falling_330.mp4', 'Falling\\\\Falling_331.mp4', 'Falling\\\\Falling_332.mp4', 'Falling\\\\Falling_333.mp4', 'Falling\\\\Falling_334.mp4', 'Falling\\\\Falling_335.mp4', 'Falling\\\\Falling_336.mp4', 'Falling\\\\Falling_337.mp4', 'Falling\\\\Falling_338.mp4', 'Falling\\\\Falling_339.mp4', 'Falling\\\\Falling_340.mp4', 'Falling\\\\Falling_341.mp4', 'Falling\\\\Falling_342.mp4', 'Falling\\\\Falling_343.mp4', 'Falling\\\\Falling_344.mp4', 'Falling\\\\Falling_345.mp4', 'Falling\\\\Falling_346.mp4', 'Falling\\\\Falling_347.mp4', 'Falling\\\\Falling_348.mp4', 'Falling\\\\Falling_349.mp4', 'Falling\\\\Falling_350.mp4', 'Falling\\\\Falling_351.mp4', 'Falling\\\\Falling_352.mp4', 'Falling\\\\Falling_353.mp4', 'Falling\\\\Falling_354.mp4', 'Falling\\\\Falling_355.mp4', 'Falling\\\\Falling_356.mp4', 'Falling\\\\Falling_357.mp4', 'Falling\\\\Falling_358.mp4', 'Falling\\\\Falling_359.mp4', 'Falling\\\\Falling_360.mp4', 'Falling\\\\Falling_361.mp4', 'Falling\\\\Falling_362.mp4', 'Falling\\\\Falling_363.mp4', 'Falling\\\\Falling_364.mp4', 'Falling\\\\Falling_365.mp4', 'Falling\\\\Falling_366.mp4', 'Falling\\\\Falling_367.mp4', 'Falling\\\\Falling_368.mp4', 'Falling\\\\Falling_369.mp4', 'Falling\\\\Falling_370.mp4', 'Falling\\\\Falling_371.mp4', 'Falling\\\\Falling_372.mp4', 'Falling\\\\Falling_373.mp4', 'Falling\\\\Falling_374.mp4', 'Falling\\\\Falling_375.mp4', 'Falling\\\\Falling_376.mp4', 'Falling\\\\Falling_377.mp4', 'Falling\\\\Falling_378.mp4', 'Falling\\\\Falling_379.mp4', 'Falling\\\\Falling_380.mp4', 'Falling\\\\Falling_381.mp4', 'Falling\\\\Falling_382.mp4', 'Falling\\\\Falling_383.mp4', 'Falling\\\\Falling_384.mp4', 'Falling\\\\Falling_385.mp4', 'Falling\\\\Falling_386.mp4', 'Falling\\\\Falling_387.mp4', 'Falling\\\\Falling_388.mp4', 'Falling\\\\Falling_389.mp4', 'Falling\\\\Falling_390.mp4', 'Falling\\\\Falling_391.mp4', 'Falling\\\\Falling_392.mp4', 'Falling\\\\Falling_393.mp4', 'Falling\\\\Falling_394.mp4', 'Falling\\\\Falling_395.mp4', 'Falling\\\\Falling_396.mp4', 'Falling\\\\Falling_397.mp4', 'Falling\\\\Falling_398.mp4', 'Falling\\\\Falling_399.mp4', 'Falling\\\\Falling_400.mp4', 'Falling\\\\Falling_401.mp4', 'Falling\\\\Falling_402.mp4', 'Falling\\\\Falling_403.mp4', 'Falling\\\\Falling_404.mp4', 'Falling\\\\Falling_405.mp4', 'Falling\\\\Falling_406.mp4', 'Falling\\\\Falling_407.mp4', 'Falling\\\\Falling_408.mp4', 'Falling\\\\Falling_409.mp4', 'Falling\\\\Falling_410.mp4', 'Falling\\\\Falling_411.mp4', 'Falling\\\\Falling_412.mp4', 'Falling\\\\Falling_413.mp4', 'Falling\\\\Falling_414.mp4', 'Falling\\\\Falling_415.mp4', 'Falling\\\\Falling_416.mp4', 'Falling\\\\Falling_417.mp4', 'Falling\\\\Falling_418.mp4', 'Falling\\\\Falling_419.mp4', 'Falling\\\\Falling_420.mp4', 'Falling\\\\Falling_421.mp4', 'Falling\\\\Falling_422.mp4', 'Falling\\\\Falling_423.mp4', 'Falling\\\\Falling_424.mp4', 'Falling\\\\Falling_425.mp4', 'Falling\\\\Falling_426.mp4', 'Falling\\\\Falling_427.mp4', 'Falling\\\\Falling_428.mp4', 'Falling\\\\Falling_429.mp4', 'Falling\\\\Falling_430.mp4', 'Falling\\\\Falling_431.mp4', 'Falling\\\\Falling_432.mp4', 'Falling\\\\Falling_433.mp4', 'Falling\\\\Falling_434.mp4', 'Falling\\\\Falling_435.mp4', 'Falling\\\\Falling_436.mp4', 'Falling\\\\Falling_437.mp4', 'Falling\\\\Falling_438.mp4', 'Falling\\\\Falling_439.mp4', 'Falling\\\\Falling_440.mp4', 'Falling\\\\Falling_441.mp4', 'Falling\\\\Falling_442.mp4', 'Falling\\\\Falling_443.mp4', 'Falling\\\\Falling_444.mp4', 'Falling\\\\Falling_445.mp4', 'Falling\\\\Falling_446.mp4', 'Falling\\\\Falling_447.mp4', 'Falling\\\\Falling_448.mp4', 'Falling\\\\Falling_449.mp4', 'Falling\\\\Falling_450.mp4', 'Falling\\\\Falling_451.mp4', 'Falling\\\\Falling_452.mp4', 'Falling\\\\Falling_453.mp4', 'Falling\\\\Falling_454.mp4', 'Falling\\\\Falling_455.mp4', 'Falling\\\\Falling_456.mp4', 'Falling\\\\Falling_457.mp4', 'Falling\\\\Falling_458.mp4', 'Falling\\\\Falling_459.mp4', 'Falling\\\\Falling_460.mp4', 'Falling\\\\Falling_461.mp4', 'Falling\\\\Falling_462.mp4', 'Falling\\\\Falling_463.mp4', 'Falling\\\\Falling_464.mp4', 'Falling\\\\Falling_465.mp4', 'Falling\\\\Falling_466.mp4', 'Falling\\\\Falling_467.mp4', 'Falling\\\\Falling_468.mp4', 'Falling\\\\Falling_469.mp4', 'Falling\\\\Falling_470.mp4', 'Falling\\\\Falling_471.mp4', 'Falling\\\\Falling_472.mp4', 'Falling\\\\Falling_473.mp4', 'Falling\\\\Falling_474.mp4', 'Falling\\\\Falling_475.mp4', 'Falling\\\\Falling_476.mp4', 'Falling\\\\Falling_477.mp4', 'Falling\\\\Falling_478.mp4', 'Falling\\\\Falling_479.mp4', 'Falling\\\\Falling_480.mp4', 'Falling\\\\Falling_481.mp4', 'Falling\\\\Falling_482.mp4', 'Falling\\\\Falling_483.mp4', 'Falling\\\\Falling_484.mp4', 'Falling\\\\Falling_485.mp4', 'Falling\\\\Falling_486.mp4', 'Falling\\\\Falling_487.mp4', 'Falling\\\\Falling_488.mp4', 'Falling\\\\Falling_489.mp4', 'Falling\\\\Falling_490.mp4', 'Falling\\\\Falling_491.mp4', 'Falling\\\\Falling_492.mp4', 'Falling\\\\Falling_493.mp4', 'Falling\\\\Falling_494.mp4', 'Falling\\\\Falling_495.mp4', 'Falling\\\\Falling_496.mp4', 'Falling\\\\Falling_497.mp4', 'Falling\\\\Falling_498.mp4', 'Falling\\\\Falling_499.mp4', 'Falling\\\\Falling_500.mp4', 'Falling\\\\Falling_501.mp4', 'Falling\\\\Falling_502.mp4', 'Falling\\\\Falling_503.mp4', 'Falling\\\\Falling_504.mp4', 'Falling\\\\Falling_505.mp4', 'Falling\\\\Falling_506.mp4', 'Falling\\\\Falling_507.mp4', 'Falling\\\\Falling_508.mp4', 'Falling\\\\Falling_509.mp4', 'Falling\\\\Falling_510.mp4', 'Falling\\\\Falling_511.mp4', 'Falling\\\\Falling_512.mp4', 'Falling\\\\Falling_513.mp4', 'Falling\\\\Falling_514.mp4', 'Falling\\\\Falling_515.mp4', 'Falling\\\\Falling_516.mp4', 'Falling\\\\Falling_517.mp4', 'Falling\\\\Falling_518.mp4', 'Falling\\\\Falling_519.mp4', 'Falling\\\\Falling_520.mp4', 'Falling\\\\Falling_521.mp4', 'Falling\\\\Falling_522.mp4', 'Falling\\\\Falling_523.mp4', 'Falling\\\\Falling_524.mp4', 'Falling\\\\Falling_525.mp4', 'Falling\\\\Falling_526.mp4', 'Falling\\\\Falling_527.mp4', 'Falling\\\\Falling_528.mp4', 'Falling\\\\Falling_529.mp4', 'Falling\\\\Falling_530.mp4', 'Falling\\\\Falling_531.mp4', 'Falling\\\\Falling_532.mp4', 'Falling\\\\Falling_533.mp4', 'Falling\\\\Falling_534.mp4', 'Falling\\\\Falling_535.mp4', 'Falling\\\\Falling_536.mp4', 'Falling\\\\Falling_537.mp4', 'Falling\\\\Falling_538.mp4', 'Falling\\\\Falling_539.mp4', 'Falling\\\\Falling_540.mp4', 'Falling\\\\Falling_541.mp4', 'Falling\\\\Falling_542.mp4', 'Falling\\\\Falling_543.mp4', 'Falling\\\\Falling_544.mp4', 'Falling\\\\Falling_545.mp4', 'Falling\\\\Falling_546.mp4', 'Falling\\\\Falling_547.mp4', 'Falling\\\\Falling_548.mp4', 'Falling\\\\Falling_549.mp4', 'Falling\\\\Falling_550.mp4', 'Falling\\\\Falling_551.mp4', 'Falling\\\\Falling_552.mp4', 'Falling\\\\Falling_553.mp4', 'Falling\\\\Falling_554.mp4', 'Falling\\\\Falling_555.mp4', 'Falling\\\\Falling_556.mp4', 'Falling\\\\Falling_557.mp4', 'Falling\\\\Falling_558.mp4', 'Falling\\\\Falling_559.mp4', 'Falling\\\\Falling_560.mp4', 'Falling\\\\Falling_561.mp4', 'Falling\\\\Falling_562.mp4', 'Falling\\\\Falling_563.mp4', 'Falling\\\\Falling_564.mp4', 'Falling\\\\Falling_565.mp4', 'Falling\\\\Falling_566.mp4', 'Falling\\\\Falling_567.mp4', 'Falling\\\\Falling_568.mp4', 'Falling\\\\Falling_569.mp4', 'Falling\\\\Falling_570.mp4', 'Falling\\\\Falling_571.mp4', 'Falling\\\\Falling_572.mp4', 'Falling\\\\Falling_573.mp4', 'Falling\\\\Falling_574.mp4', 'Falling\\\\Falling_575.mp4', 'Falling\\\\Falling_576.mp4', 'Falling\\\\Falling_577.mp4', 'Falling\\\\Falling_578.mp4', 'Falling\\\\Falling_579.mp4', 'Falling\\\\Falling_580.mp4', 'Falling\\\\Falling_581.mp4', 'Falling\\\\Falling_582.mp4', 'Falling\\\\Falling_583.mp4', 'Falling\\\\Falling_584.mp4', 'Falling\\\\Falling_585.mp4', 'Walking\\\\Walking_0.mp4', 'Walking\\\\Walking_1.mp4', 'Walking\\\\Walking_2.mp4', 'Walking\\\\Walking_3.mp4', 'Walking\\\\Walking_4.mp4', 'Walking\\\\Walking_5.mp4', 'Walking\\\\Walking_6.mp4', 'Walking\\\\Walking_7.mp4', 'Walking\\\\Walking_8.mp4', 'Walking\\\\Walking_9.mp4', 'Walking\\\\Walking_10.mp4', 'Walking\\\\Walking_11.mp4', 'Walking\\\\Walking_12.mp4', 'Walking\\\\Walking_13.mp4', 'Walking\\\\Walking_14.mp4', 'Walking\\\\Walking_15.mp4', 'Walking\\\\Walking_16.mp4', 'Walking\\\\Walking_17.mp4', 'Walking\\\\Walking_18.mp4', 'Walking\\\\Walking_19.mp4', 'Walking\\\\Walking_20.mp4', 'Walking\\\\Walking_21.mp4', 'Walking\\\\Walking_22.mp4', 'Walking\\\\Walking_23.mp4', 'Walking\\\\Walking_24.mp4', 'Walking\\\\Walking_25.mp4', 'Walking\\\\Walking_26.mp4', 'Walking\\\\Walking_27.mp4', 'Walking\\\\Walking_28.mp4', 'Walking\\\\Walking_29.mp4', 'Walking\\\\Walking_30.mp4', 'Walking\\\\Walking_31.mp4', 'Walking\\\\Walking_32.mp4', 'Walking\\\\Walking_33.mp4', 'Walking\\\\Walking_34.mp4', 'Walking\\\\Walking_35.mp4', 'Walking\\\\Walking_36.mp4', 'Walking\\\\Walking_37.mp4', 'Walking\\\\Walking_38.mp4', 'Walking\\\\Walking_39.mp4', 'Walking\\\\Walking_40.mp4', 'Walking\\\\Walking_41.mp4', 'Walking\\\\Walking_42.mp4', 'Walking\\\\Walking_43.mp4', 'Walking\\\\Walking_44.mp4', 'Walking\\\\Walking_45.mp4', 'Walking\\\\Walking_46.mp4', 'Walking\\\\Walking_47.mp4', 'Walking\\\\Walking_48.mp4', 'Walking\\\\Walking_49.mp4', 'Walking\\\\Walking_50.mp4', 'Walking\\\\Walking_51.mp4', 'Walking\\\\Walking_52.mp4', 'Walking\\\\Walking_53.mp4', 'Walking\\\\Walking_54.mp4', 'Walking\\\\Walking_55.mp4', 'Walking\\\\Walking_56.mp4', 'Walking\\\\Walking_57.mp4', 'Walking\\\\Walking_58.mp4', 'Walking\\\\Walking_59.mp4', 'Walking\\\\Walking_60.mp4', 'Walking\\\\Walking_61.mp4', 'Walking\\\\Walking_62.mp4', 'Walking\\\\Walking_63.mp4', 'Walking\\\\Walking_64.mp4', 'Walking\\\\Walking_65.mp4', 'Walking\\\\Walking_66.mp4', 'Walking\\\\Walking_67.mp4', 'Walking\\\\Walking_68.mp4', 'Walking\\\\Walking_69.mp4', 'Walking\\\\Walking_70.mp4', 'Walking\\\\Walking_71.mp4', 'Walking\\\\Walking_72.mp4', 'Walking\\\\Walking_73.mp4', 'Walking\\\\Walking_74.mp4', 'Walking\\\\Walking_75.mp4', 'Walking\\\\Walking_76.mp4', 'Walking\\\\Walking_77.mp4', 'Walking\\\\Walking_78.mp4', 'Walking\\\\Walking_79.mp4', 'Walking\\\\Walking_80.mp4', 'Walking\\\\Walking_81.mp4', 'Walking\\\\Walking_82.mp4', 'Walking\\\\Walking_83.mp4', 'Walking\\\\Walking_84.mp4', 'Walking\\\\Walking_85.mp4', 'Walking\\\\Walking_86.mp4', 'Walking\\\\Walking_87.mp4', 'Walking\\\\Walking_88.mp4', 'Walking\\\\Walking_89.mp4', 'Walking\\\\Walking_90.mp4', 'Walking\\\\Walking_91.mp4', 'Walking\\\\Walking_92.mp4', 'Walking\\\\Walking_93.mp4', 'Walking\\\\Walking_94.mp4', 'Walking\\\\Walking_95.mp4', 'Walking\\\\Walking_96.mp4', 'Walking\\\\Walking_97.mp4', 'Walking\\\\Walking_98.mp4', 'Walking\\\\Walking_99.mp4', 'Walking\\\\Walking_100.mp4', 'Walking\\\\Walking_101.mp4', 'Walking\\\\Walking_102.mp4', 'Walking\\\\Walking_103.mp4', 'Walking\\\\Walking_104.mp4', 'Walking\\\\Walking_105.mp4', 'Walking\\\\Walking_106.mp4', 'Walking\\\\Walking_107.mp4', 'Walking\\\\Walking_108.mp4', 'Walking\\\\Walking_109.mp4', 'Walking\\\\Walking_110.mp4', 'Walking\\\\Walking_111.mp4', 'Walking\\\\Walking_112.mp4', 'Walking\\\\Walking_113.mp4', 'Walking\\\\Walking_114.mp4', 'Walking\\\\Walking_115.mp4', 'Walking\\\\Walking_116.mp4', 'Walking\\\\Walking_117.mp4', 'Walking\\\\Walking_118.mp4', 'Walking\\\\Walking_119.mp4', 'Walking\\\\Walking_120.mp4', 'Walking\\\\Walking_121.mp4', 'Walking\\\\Walking_122.mp4', 'Walking\\\\Walking_123.mp4', 'Walking\\\\Walking_124.mp4', 'Walking\\\\Walking_125.mp4', 'Walking\\\\Walking_126.mp4', 'Walking\\\\Walking_127.mp4', 'Walking\\\\Walking_128.mp4', 'Walking\\\\Walking_129.mp4', 'Walking\\\\Walking_130.mp4', 'Walking\\\\Walking_131.mp4', 'Walking\\\\Walking_132.mp4', 'Walking\\\\Walking_133.mp4', 'Walking\\\\Walking_134.mp4', 'Walking\\\\Walking_135.mp4', 'Walking\\\\Walking_136.mp4', 'Walking\\\\Walking_137.mp4', 'Walking\\\\Walking_138.mp4', 'Walking\\\\Walking_139.mp4', 'Walking\\\\Walking_140.mp4', 'Walking\\\\Walking_141.mp4', 'Walking\\\\Walking_142.mp4', 'Walking\\\\Walking_143.mp4', 'Walking\\\\Walking_144.mp4', 'Walking\\\\Walking_145.mp4', 'Walking\\\\Walking_146.mp4', 'Walking\\\\Walking_147.mp4', 'Walking\\\\Walking_148.mp4', 'Walking\\\\Walking_149.mp4', 'Walking\\\\Walking_150.mp4', 'Walking\\\\Walking_151.mp4', 'Walking\\\\Walking_152.mp4', 'Walking\\\\Walking_153.mp4', 'Walking\\\\Walking_154.mp4', 'Walking\\\\Walking_155.mp4', 'Walking\\\\Walking_156.mp4', 'Walking\\\\Walking_157.mp4', 'Walking\\\\Walking_158.mp4', 'Walking\\\\Walking_159.mp4', 'Walking\\\\Walking_160.mp4', 'Walking\\\\Walking_161.mp4', 'Walking\\\\Walking_162.mp4', 'Walking\\\\Walking_163.mp4', 'Walking\\\\Walking_164.mp4', 'Walking\\\\Walking_165.mp4', 'Walking\\\\Walking_166.mp4', 'Walking\\\\Walking_167.mp4', 'Walking\\\\Walking_168.mp4', 'Walking\\\\Walking_169.mp4', 'Walking\\\\Walking_170.mp4', 'Walking\\\\Walking_171.mp4', 'Walking\\\\Walking_172.mp4', 'Walking\\\\Walking_173.mp4', 'Walking\\\\Walking_174.mp4', 'Walking\\\\Walking_175.mp4', 'Walking\\\\Walking_176.mp4', 'Walking\\\\Walking_177.mp4', 'Walking\\\\Walking_178.mp4', 'Walking\\\\Walking_179.mp4', 'Walking\\\\Walking_180.mp4', 'Walking\\\\Walking_181.mp4', 'Walking\\\\Walking_182.mp4', 'Walking\\\\Walking_183.mp4', 'Walking\\\\Walking_184.mp4', 'Walking\\\\Walking_185.mp4', 'Walking\\\\Walking_186.mp4', 'Walking\\\\Walking_187.mp4', 'Walking\\\\Walking_188.mp4', 'Walking\\\\Walking_189.mp4', 'Walking\\\\Walking_190.mp4', 'Walking\\\\Walking_191.mp4', 'Walking\\\\Walking_192.mp4', 'Walking\\\\Walking_193.mp4', 'Walking\\\\Walking_194.mp4', 'Walking\\\\Walking_195.mp4', 'Walking\\\\Walking_196.mp4', 'Walking\\\\Walking_197.mp4', 'Walking\\\\Walking_198.mp4', 'Walking\\\\Walking_199.mp4', 'Walking\\\\Walking_200.mp4', 'Walking\\\\Walking_201.mp4', 'Walking\\\\Walking_202.mp4', 'Walking\\\\Walking_203.mp4', 'Walking\\\\Walking_204.mp4', 'Walking\\\\Walking_205.mp4', 'Walking\\\\Walking_206.mp4', 'Walking\\\\Walking_207.mp4', 'Walking\\\\Walking_208.mp4', 'Walking\\\\Walking_209.mp4', 'Walking\\\\Walking_210.mp4', 'Walking\\\\Walking_211.mp4', 'Walking\\\\Walking_212.mp4', 'Walking\\\\Walking_213.mp4', 'Walking\\\\Walking_214.mp4', 'Walking\\\\Walking_215.mp4', 'Walking\\\\Walking_216.mp4', 'Walking\\\\Walking_217.mp4', 'Walking\\\\Walking_218.mp4', 'Walking\\\\Walking_219.mp4', 'Walking\\\\Walking_220.mp4', 'Walking\\\\Walking_221.mp4', 'Walking\\\\Walking_222.mp4', 'Walking\\\\Walking_223.mp4', 'Walking\\\\Walking_224.mp4', 'Walking\\\\Walking_225.mp4', 'Walking\\\\Walking_226.mp4', 'Walking\\\\Walking_227.mp4', 'Walking\\\\Walking_228.mp4', 'Walking\\\\Walking_229.mp4', 'Walking\\\\Walking_230.mp4', 'Walking\\\\Walking_231.mp4', 'Walking\\\\Walking_232.mp4', 'Walking\\\\Walking_233.mp4', 'Walking\\\\Walking_234.mp4', 'Walking\\\\Walking_235.mp4', 'Walking\\\\Walking_236.mp4', 'Walking\\\\Walking_237.mp4', 'Walking\\\\Walking_238.mp4', 'Walking\\\\Walking_239.mp4', 'Walking\\\\Walking_240.mp4', 'Walking\\\\Walking_241.mp4', 'Walking\\\\Walking_242.mp4', 'Walking\\\\Walking_243.mp4', 'Walking\\\\Walking_244.mp4', 'Walking\\\\Walking_245.mp4', 'Walking\\\\Walking_246.mp4', 'Walking\\\\Walking_247.mp4', 'Walking\\\\Walking_248.mp4', 'Walking\\\\Walking_249.mp4', 'Walking\\\\Walking_250.mp4', 'Walking\\\\Walking_251.mp4', 'Walking\\\\Walking_252.mp4', 'Walking\\\\Walking_253.mp4', 'Walking\\\\Walking_254.mp4', 'Walking\\\\Walking_255.mp4', 'Walking\\\\Walking_256.mp4', 'Walking\\\\Walking_257.mp4', 'Walking\\\\Walking_258.mp4', 'Walking\\\\Walking_259.mp4', 'Walking\\\\Walking_260.mp4', 'Walking\\\\Walking_261.mp4', 'Walking\\\\Walking_262.mp4', 'Walking\\\\Walking_263.mp4', 'Walking\\\\Walking_264.mp4', 'Walking\\\\Walking_265.mp4', 'Walking\\\\Walking_266.mp4', 'Walking\\\\Walking_267.mp4', 'Walking\\\\Walking_268.mp4', 'Walking\\\\Walking_269.mp4', 'Walking\\\\Walking_270.mp4', 'Walking\\\\Walking_271.mp4', 'Walking\\\\Walking_272.mp4', 'Walking\\\\Walking_273.mp4', 'Walking\\\\Walking_274.mp4', 'Walking\\\\Walking_275.mp4', 'Walking\\\\Walking_276.mp4', 'Walking\\\\Walking_277.mp4', 'Walking\\\\Walking_278.mp4', 'Walking\\\\Walking_279.mp4', 'Walking\\\\Walking_280.mp4', 'Walking\\\\Walking_281.mp4', 'Walking\\\\Walking_282.mp4', 'Walking\\\\Walking_283.mp4', 'Walking\\\\Walking_284.mp4', 'Walking\\\\Walking_285.mp4', 'Walking\\\\Walking_286.mp4', 'Walking\\\\Walking_287.mp4', 'Walking\\\\Walking_288.mp4', 'Walking\\\\Walking_289.mp4', 'Walking\\\\Walking_290.mp4', 'Walking\\\\Walking_291.mp4', 'Walking\\\\Walking_292.mp4', 'Walking\\\\Walking_293.mp4', 'Walking\\\\Walking_294.mp4', 'Walking\\\\Walking_295.mp4', 'Walking\\\\Walking_296.mp4', 'Walking\\\\Walking_297.mp4', 'Walking\\\\Walking_298.mp4', 'Walking\\\\Walking_299.mp4', 'Walking\\\\Walking_300.mp4', 'Walking\\\\Walking_301.mp4', 'Walking\\\\Walking_302.mp4', 'Walking\\\\Walking_303.mp4', 'Walking\\\\Walking_304.mp4', 'Walking\\\\Walking_305.mp4', 'Walking\\\\Walking_306.mp4', 'Walking\\\\Walking_307.mp4', 'Walking\\\\Walking_308.mp4', 'Walking\\\\Walking_309.mp4', 'Walking\\\\Walking_310.mp4', 'Walking\\\\Walking_311.mp4', 'Walking\\\\Walking_312.mp4', 'Walking\\\\Walking_313.mp4', 'Walking\\\\Walking_314.mp4', 'Walking\\\\Walking_315.mp4', 'Walking\\\\Walking_316.mp4', 'Walking\\\\Walking_317.mp4', 'Walking\\\\Walking_318.mp4', 'Walking\\\\Walking_319.mp4', 'Walking\\\\Walking_320.mp4', 'Walking\\\\Walking_321.mp4', 'Walking\\\\Walking_322.mp4', 'Walking\\\\Walking_323.mp4', 'Walking\\\\Walking_324.mp4', 'Walking\\\\Walking_325.mp4', 'Walking\\\\Walking_326.mp4', 'Walking\\\\Walking_327.mp4', 'Walking\\\\Walking_328.mp4', 'Walking\\\\Walking_329.mp4', 'Walking\\\\Walking_330.mp4', 'Walking\\\\Walking_331.mp4', 'Walking\\\\Walking_332.mp4', 'Walking\\\\Walking_333.mp4', 'Walking\\\\Walking_334.mp4', 'Walking\\\\Walking_335.mp4', 'Walking\\\\Walking_336.mp4', 'Walking\\\\Walking_337.mp4', 'Walking\\\\Walking_338.mp4', 'Walking\\\\Walking_339.mp4', 'Walking\\\\Walking_340.mp4', 'Walking\\\\Walking_341.mp4', 'Walking\\\\Walking_342.mp4', 'Walking\\\\Walking_343.mp4', 'Walking\\\\Walking_344.mp4', 'Walking\\\\Walking_345.mp4', 'Walking\\\\Walking_346.mp4', 'Walking\\\\Walking_347.mp4', 'Walking\\\\Walking_348.mp4', 'Walking\\\\Walking_349.mp4', 'Walking\\\\Walking_350.mp4', 'Walking\\\\Walking_351.mp4', 'Walking\\\\Walking_352.mp4', 'Walking\\\\Walking_353.mp4', 'Walking\\\\Walking_354.mp4', 'Walking\\\\Walking_355.mp4', 'Walking\\\\Walking_356.mp4', 'Walking\\\\Walking_357.mp4', 'Walking\\\\Walking_358.mp4', 'Walking\\\\Walking_359.mp4', 'Walking\\\\Walking_360.mp4', 'Walking\\\\Walking_361.mp4', 'Walking\\\\Walking_362.mp4', 'Walking\\\\Walking_363.mp4', 'Walking\\\\Walking_364.mp4', 'Walking\\\\Walking_365.mp4', 'Walking\\\\Walking_366.mp4', 'Walking\\\\Walking_367.mp4', 'Walking\\\\Walking_368.mp4', 'Walking\\\\Walking_369.mp4', 'Walking\\\\Walking_370.mp4', 'Walking\\\\Walking_371.mp4', 'Walking\\\\Walking_372.mp4', 'Walking\\\\Walking_373.mp4', 'Walking\\\\Walking_374.mp4', 'Walking\\\\Walking_375.mp4', 'Walking\\\\Walking_376.mp4', 'Walking\\\\Walking_377.mp4', 'Walking\\\\Walking_378.mp4', 'Walking\\\\Walking_379.mp4', 'Walking\\\\Walking_380.mp4', 'Walking\\\\Walking_381.mp4', 'Walking\\\\Walking_382.mp4', 'Walking\\\\Walking_383.mp4', 'Walking\\\\Walking_384.mp4', 'Walking\\\\Walking_385.mp4', 'Walking\\\\Walking_386.mp4', 'Walking\\\\Walking_387.mp4', 'Walking\\\\Walking_388.mp4', 'Walking\\\\Walking_389.mp4', 'Walking\\\\Walking_390.mp4', 'Walking\\\\Walking_391.mp4', 'Walking\\\\Walking_392.mp4', 'Walking\\\\Walking_393.mp4', 'Walking\\\\Walking_394.mp4', 'Walking\\\\Walking_395.mp4', 'Walking\\\\Walking_396.mp4', 'Walking\\\\Walking_397.mp4', 'Walking\\\\Walking_398.mp4', 'Walking\\\\Walking_399.mp4', 'Walking\\\\Walking_400.mp4', 'Walking\\\\Walking_401.mp4', 'Walking\\\\Walking_402.mp4', 'Walking\\\\Walking_403.mp4', 'Walking\\\\Walking_404.mp4', 'Walking\\\\Walking_405.mp4', 'Walking\\\\Walking_406.mp4', 'Walking\\\\Walking_407.mp4', 'Walking\\\\Walking_408.mp4', 'Walking\\\\Walking_409.mp4', 'Walking\\\\Walking_410.mp4', 'Walking\\\\Walking_411.mp4', 'Walking\\\\Walking_412.mp4', 'Walking\\\\Walking_413.mp4', 'Walking\\\\Walking_414.mp4', 'Walking\\\\Walking_415.mp4', 'Walking\\\\Walking_416.mp4', 'Walking\\\\Walking_417.mp4', 'Walking\\\\Walking_418.mp4', 'Walking\\\\Walking_419.mp4', 'Walking\\\\Walking_420.mp4', 'Walking\\\\Walking_421.mp4', 'Walking\\\\Walking_422.mp4', 'Walking\\\\Walking_423.mp4', 'Walking\\\\Walking_424.mp4', 'Walking\\\\Walking_425.mp4', 'Walking\\\\Walking_426.mp4', 'Walking\\\\Walking_427.mp4', 'Walking\\\\Walking_428.mp4', 'Walking\\\\Walking_429.mp4', 'Walking\\\\Walking_430.mp4', 'Walking\\\\Walking_431.mp4', 'Walking\\\\Walking_432.mp4', 'Walking\\\\Walking_433.mp4', 'Walking\\\\Walking_434.mp4', 'Walking\\\\Walking_435.mp4', 'Walking\\\\Walking_436.mp4', 'Walking\\\\Walking_437.mp4', 'Walking\\\\Walking_438.mp4', 'Walking\\\\Walking_439.mp4', 'Walking\\\\Walking_440.mp4', 'Walking\\\\Walking_441.mp4', 'Walking\\\\Walking_442.mp4', 'Walking\\\\Walking_443.mp4', 'Walking\\\\Walking_444.mp4', 'Walking\\\\Walking_445.mp4', 'Walking\\\\Walking_446.mp4', 'Walking\\\\Walking_447.mp4', 'Walking\\\\Walking_448.mp4', 'Walking\\\\Walking_449.mp4', 'Walking\\\\Walking_450.mp4', 'Walking\\\\Walking_451.mp4', 'Walking\\\\Walking_452.mp4', 'Walking\\\\Walking_453.mp4', 'Walking\\\\Walking_454.mp4', 'Walking\\\\Walking_455.mp4', 'Walking\\\\Walking_456.mp4', 'Walking\\\\Walking_457.mp4', 'Walking\\\\Walking_458.mp4', 'Walking\\\\Walking_459.mp4', 'Walking\\\\Walking_460.mp4', 'Walking\\\\Walking_461.mp4', 'Walking\\\\Walking_462.mp4', 'Walking\\\\Walking_463.mp4', 'Walking\\\\Walking_464.mp4', 'Walking\\\\Walking_465.mp4', 'Walking\\\\Walking_466.mp4', 'Walking\\\\Walking_467.mp4', 'Walking\\\\Walking_468.mp4', 'Walking\\\\Walking_469.mp4', 'Walking\\\\Walking_470.mp4', 'Walking\\\\Walking_471.mp4', 'Walking\\\\Walking_472.mp4', 'Walking\\\\Walking_473.mp4', 'Walking\\\\Walking_474.mp4', 'Walking\\\\Walking_475.mp4', 'Walking\\\\Walking_476.mp4', 'Walking\\\\Walking_477.mp4', 'Walking\\\\Walking_478.mp4', 'Walking\\\\Walking_479.mp4', 'Walking\\\\Walking_480.mp4', 'Walking\\\\Walking_481.mp4', 'Walking\\\\Walking_482.mp4', 'Walking\\\\Walking_483.mp4', 'Walking\\\\Walking_484.mp4', 'Walking\\\\Walking_485.mp4', 'Walking\\\\Walking_486.mp4', 'Walking\\\\Walking_487.mp4', 'Walking\\\\Walking_488.mp4', 'Walking\\\\Walking_489.mp4', 'Walking\\\\Walking_490.mp4', 'Walking\\\\Walking_491.mp4', 'Walking\\\\Walking_492.mp4', 'Walking\\\\Walking_493.mp4', 'Walking\\\\Walking_494.mp4', 'Walking\\\\Walking_495.mp4', 'Walking\\\\Walking_496.mp4', 'Walking\\\\Walking_497.mp4', 'Walking\\\\Walking_498.mp4', 'Walking\\\\Walking_499.mp4', 'Walking\\\\Walking_500.mp4', 'Walking\\\\Walking_501.mp4', 'Walking\\\\Walking_502.mp4', 'Walking\\\\Walking_503.mp4', 'Walking\\\\Walking_504.mp4', 'Walking\\\\Walking_505.mp4', 'Walking\\\\Walking_506.mp4', 'Walking\\\\Walking_507.mp4', 'Walking\\\\Walking_508.mp4', 'Walking\\\\Walking_509.mp4', 'Walking\\\\Walking_510.mp4', 'Walking\\\\Walking_511.mp4', 'Walking\\\\Walking_512.mp4', 'Walking\\\\Walking_513.mp4', 'Walking\\\\Walking_514.mp4', 'Walking\\\\Walking_515.mp4', 'Walking\\\\Walking_516.mp4', 'Walking\\\\Walking_517.mp4', 'Walking\\\\Walking_518.mp4', 'Walking\\\\Walking_519.mp4', 'Walking\\\\Walking_520.mp4', 'Walking\\\\Walking_521.mp4', 'Walking\\\\Walking_522.mp4', 'Walking\\\\Walking_523.mp4', 'Walking\\\\Walking_524.mp4', 'Walking\\\\Walking_525.mp4', 'Walking\\\\Walking_526.mp4', 'Walking\\\\Walking_527.mp4', 'Walking\\\\Walking_528.mp4', 'Walking\\\\Walking_529.mp4', 'Walking\\\\Walking_530.mp4', 'Walking\\\\Walking_531.mp4', 'Walking\\\\Walking_532.mp4', 'Walking\\\\Walking_533.mp4', 'Walking\\\\Walking_534.mp4', 'Walking\\\\Walking_535.mp4', 'Walking\\\\Walking_536.mp4', 'Walking\\\\Walking_537.mp4', 'Walking\\\\Walking_538.mp4', 'Walking\\\\Walking_539.mp4', 'Walking\\\\Walking_540.mp4', 'Walking\\\\Walking_541.mp4', 'Walking\\\\Walking_542.mp4', 'Walking\\\\Walking_543.mp4', 'Walking\\\\Walking_544.mp4', 'Walking\\\\Walking_545.mp4', 'Walking\\\\Walking_546.mp4', 'Walking\\\\Walking_547.mp4', 'Walking\\\\Walking_548.mp4', 'Walking\\\\Walking_549.mp4', 'Walking\\\\Walking_550.mp4', 'Walking\\\\Walking_551.mp4', 'Walking\\\\Walking_552.mp4', 'Walking\\\\Walking_553.mp4', 'Walking\\\\Walking_554.mp4', 'Walking\\\\Walking_555.mp4', 'Walking\\\\Walking_556.mp4', 'Walking\\\\Walking_557.mp4', 'Walking\\\\Walking_558.mp4', 'Walking\\\\Walking_559.mp4', 'Walking\\\\Walking_560.mp4', 'Walking\\\\Walking_561.mp4', 'Walking\\\\Walking_562.mp4', 'Walking\\\\Walking_563.mp4', 'Walking\\\\Walking_564.mp4', 'Walking\\\\Walking_565.mp4', 'Walking\\\\Walking_566.mp4', 'Walking\\\\Walking_567.mp4', 'Walking\\\\Walking_568.mp4', 'Walking\\\\Walking_569.mp4', 'Walking\\\\Walking_570.mp4', 'Walking\\\\Walking_571.mp4', 'Walking\\\\Walking_572.mp4', 'Walking\\\\Walking_573.mp4', 'Walking\\\\Walking_574.mp4', 'Walking\\\\Walking_575.mp4', 'Walking\\\\Walking_576.mp4', 'Walking\\\\Walking_577.mp4', 'Walking\\\\Walking_578.mp4', 'Walking\\\\Walking_579.mp4', 'Walking\\\\Walking_580.mp4', 'Walking\\\\Walking_581.mp4', 'Walking\\\\Walking_582.mp4', 'Walking\\\\Walking_583.mp4', 'Walking\\\\Walking_584.mp4', 'Walking\\\\Walking_585.mp4', 'Walking\\\\Walking_586.mp4', 'Walking\\\\Walking_587.mp4', 'Walking\\\\Walking_588.mp4', 'Walking\\\\Walking_589.mp4', 'Walking\\\\Walking_590.mp4', 'Walking\\\\Walking_591.mp4', 'Walking\\\\Walking_592.mp4', 'Walking\\\\Walking_593.mp4', 'Walking\\\\Walking_594.mp4', 'Walking\\\\Walking_595.mp4', 'Walking\\\\Walking_596.mp4', 'Walking\\\\Walking_597.mp4', 'Walking\\\\Walking_598.mp4', 'Walking\\\\Walking_599.mp4', 'Walking\\\\Walking_600.mp4', 'Walking\\\\Walking_601.mp4', 'Walking\\\\Walking_602.mp4', 'Walking\\\\Walking_603.mp4', 'Walking\\\\Walking_604.mp4', 'Walking\\\\Walking_605.mp4', 'Walking\\\\Walking_606.mp4', 'Walking\\\\Walking_607.mp4', 'Walking\\\\Walking_608.mp4', 'Walking\\\\Walking_609.mp4', 'Walking\\\\Walking_610.mp4', 'Walking\\\\Walking_611.mp4', 'Walking\\\\Walking_612.mp4', 'Walking\\\\Walking_613.mp4', 'Walking\\\\Walking_614.mp4', 'Walking\\\\Walking_615.mp4', 'Walking\\\\Walking_616.mp4', 'Walking\\\\Walking_617.mp4', 'Walking\\\\Walking_618.mp4', 'Walking\\\\Walking_619.mp4', 'Walking\\\\Walking_620.mp4', 'Walking\\\\Walking_621.mp4', 'Walking\\\\Walking_622.mp4']\n"
     ]
    }
   ],
   "source": [
    "print(test_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 10: \n",
    "Setup the train_dataset and valid_dataset (validation/testing).   Here we setting up training batch sets of 16.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for tesnorflow 2.*\n",
    "train_dataset = tf.data.Dataset.from_generator(make_generator(train_list),\n",
    "                output_types=(tf.float32, tf.int16),\n",
    "                output_shapes=((SEQUENCE_LENGTH, 1280), (len(LABELS))))\n",
    "                 \n",
    "\n",
    "train_dataset = train_dataset.batch(16,drop_remainder=True).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "\n",
    "valid_dataset = tf.data.Dataset.from_generator(make_generator(test_list),\n",
    "                 output_types=(tf.float32, tf.int16),\n",
    "                 output_shapes=((SEQUENCE_LENGTH, 1280), (len(LABELS))))\n",
    "valid_dataset = valid_dataset.batch(16,drop_remainder=True).prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for tensorflow 1.1.4\n",
    "train_dataset = tf.data.Dataset.from_generator(make_generator(train_list),\n",
    "                 output_types=(tf.float32, tf.int16),\n",
    "                 output_shapes=(tf.TensorShape([SEQUENCE_LENGTH, 1280]), tf.TensorShape([len(LABELS)])))\n",
    "\n",
    "train_dataset = train_dataset.batch(16,drop_remainder=True).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "\n",
    "valid_dataset = tf.data.Dataset.from_generator(make_generator(test_list),\n",
    "                 output_types=(tf.float32, tf.int16),\n",
    "                 output_shapes=(tf.TensorShape([SEQUENCE_LENGTH, 1280]), tf.TensorShape([len(LABELS)])))\n",
    "valid_dataset = valid_dataset.batch(16,drop_remainder=True).prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<DatasetV1Adapter shapes: ((16, 40, 1280), (16, 2)), types: (tf.float32, tf.int16)>\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<DatasetV1Adapter shapes: ((16, 40, 1280), (16, 2)), types: (tf.float32, tf.int16)>\n"
     ]
    }
   ],
   "source": [
    "print(valid_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mylog directory = C:/Users/STSC.LNVO-126908.000/Desktop/FallDetection\\train_log\n"
     ]
    }
   ],
   "source": [
    "BASE_DATA_PATH = 'C:/Users/STSC.LNVO-126908.000/Desktop/FallDetection'\n",
    "mylog_dir = os.path.join( BASE_DATA_PATH, \"train_log\")\n",
    "print(\"Mylog directory = \" + mylog_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnknownError",
     "evalue": "FileNotFoundError: [Errno 2] No such file or directory: 'C:\\\\Users\\\\STSC.LNVO-126908.000\\\\Desktop\\\\FallDetection\\\\Classes\\\\Falling\\\\Falling_479.npy'\nTraceback (most recent call last):\n\n  File \"C:\\Users\\STSC.LNVO-126908.000\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\script_ops.py\", line 221, in __call__\n    ret = func(*args)\n\n  File \"C:\\Users\\STSC.LNVO-126908.000\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\data\\ops\\dataset_ops.py\", line 585, in generator_py_func\n    values = next(generator_state.get_iterator(iterator_id))\n\n  File \"<ipython-input-14-b441973f18d5>\", line 20, in generator\n    features = np.load(full_path)\n\n  File \"C:\\Users\\STSC.LNVO-126908.000\\Anaconda3\\lib\\site-packages\\numpy\\lib\\npyio.py\", line 422, in load\n    fid = open(os_fspath(file), \"rb\")\n\nFileNotFoundError: [Errno 2] No such file or directory: 'C:\\\\Users\\\\STSC.LNVO-126908.000\\\\Desktop\\\\FallDetection\\\\Classes\\\\Falling\\\\Falling_479.npy'\n\n\n\t [[{{node PyFunc}}]] [Op:IteratorGetNextSync]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnknownError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-37-a48889e2fb34>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mtensorboard_callback\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensorBoard\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'tmp'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mupdate_freq\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m#model.fit(train_dataset, epochs=1, callbacks=[tensorboard_callback], validation_data=valid_dataset)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalid_dataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    727\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 728\u001b[1;33m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    729\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    730\u001b[0m   def evaluate(self,\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[0;32m    222\u001b[0m           \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    223\u001b[0m           \u001b[0mvalidation_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 224\u001b[1;33m           distribution_strategy=strategy)\n\u001b[0m\u001b[0;32m    225\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    226\u001b[0m       \u001b[0mtotal_samples\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_get_total_number_of_samples\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraining_data_adapter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36m_process_training_inputs\u001b[1;34m(model, x, y, batch_size, epochs, sample_weights, class_weights, steps_per_epoch, validation_split, validation_data, validation_steps, shuffle, distribution_strategy, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m    545\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    546\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 547\u001b[1;33m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    548\u001b[0m     \u001b[0mval_adapter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36m_process_inputs\u001b[1;34m(model, x, y, batch_size, epochs, sample_weights, class_weights, shuffle, steps, distribution_strategy, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m    592\u001b[0m         \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    593\u001b[0m         \u001b[0mcheck_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 594\u001b[1;33m         steps=steps)\n\u001b[0m\u001b[0;32m    595\u001b[0m   adapter = adapter_cls(\n\u001b[0;32m    596\u001b[0m       \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle, extract_tensors_from_dataset)\u001b[0m\n\u001b[0;32m   2417\u001b[0m     \u001b[1;31m# First, we build the model on the fly if necessary.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2418\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2419\u001b[1;33m       \u001b[0mall_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_input\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdict_inputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_build_model_with_inputs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2420\u001b[0m       \u001b[0mis_build_called\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2421\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_build_model_with_inputs\u001b[1;34m(self, inputs, targets)\u001b[0m\n\u001b[0;32m   2576\u001b[0m     \u001b[1;31m# tensors from the iterator and then standardize them.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2577\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mdataset_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDatasetV1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDatasetV2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2578\u001b[1;33m       \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtraining_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextract_tensors_from_dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2579\u001b[0m     \u001b[1;31m# We type-check that `inputs` and `targets` are either single arrays\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2580\u001b[0m     \u001b[1;31m# or lists of arrays, and extract a flat list of inputs from the passed\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_utils.py\u001b[0m in \u001b[0;36mextract_tensors_from_dataset\u001b[1;34m(dataset)\u001b[0m\n\u001b[0;32m   1580\u001b[0m   \"\"\"\n\u001b[0;32m   1581\u001b[0m   \u001b[0miterator\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_iterator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1582\u001b[1;33m   \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0munpack_iterator_input\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1583\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1584\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_utils.py\u001b[0m in \u001b[0;36munpack_iterator_input\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m   1594\u001b[0m   \"\"\"\n\u001b[0;32m   1595\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1596\u001b[1;33m     \u001b[0mnext_element\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_next\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1597\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1598\u001b[0m     raise RuntimeError('Your dataset iterator ran out of data; '\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\data\\ops\\iterator_ops.py\u001b[0m in \u001b[0;36mget_next\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m    735\u001b[0m     \"\"\"\n\u001b[0;32m    736\u001b[0m     \u001b[1;32mdel\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 737\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_internal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    738\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    739\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_gather_saveables_for_checkpoint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\data\\ops\\iterator_ops.py\u001b[0m in \u001b[0;36m_next_internal\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    649\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterator_resource\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    650\u001b[0m             \u001b[0moutput_types\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_flat_output_types\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 651\u001b[1;33m             output_shapes=self._flat_output_shapes)\n\u001b[0m\u001b[0;32m    652\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    653\u001b[0m       \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\gen_dataset_ops.py\u001b[0m in \u001b[0;36miterator_get_next_sync\u001b[1;34m(iterator, output_types, output_shapes, name)\u001b[0m\n\u001b[0;32m   2670\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2671\u001b[0m         \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2672\u001b[1;33m       \u001b[0m_six\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2673\u001b[0m   \u001b[1;31m# Add nodes to the TensorFlow graph.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2674\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_types\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\six.py\u001b[0m in \u001b[0;36mraise_from\u001b[1;34m(value, from_value)\u001b[0m\n",
      "\u001b[1;31mUnknownError\u001b[0m: FileNotFoundError: [Errno 2] No such file or directory: 'C:\\\\Users\\\\STSC.LNVO-126908.000\\\\Desktop\\\\FallDetection\\\\Classes\\\\Falling\\\\Falling_479.npy'\nTraceback (most recent call last):\n\n  File \"C:\\Users\\STSC.LNVO-126908.000\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\script_ops.py\", line 221, in __call__\n    ret = func(*args)\n\n  File \"C:\\Users\\STSC.LNVO-126908.000\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\data\\ops\\dataset_ops.py\", line 585, in generator_py_func\n    values = next(generator_state.get_iterator(iterator_id))\n\n  File \"<ipython-input-14-b441973f18d5>\", line 20, in generator\n    features = np.load(full_path)\n\n  File \"C:\\Users\\STSC.LNVO-126908.000\\Anaconda3\\lib\\site-packages\\numpy\\lib\\npyio.py\", line 422, in load\n    fid = open(os_fspath(file), \"rb\")\n\nFileNotFoundError: [Errno 2] No such file or directory: 'C:\\\\Users\\\\STSC.LNVO-126908.000\\\\Desktop\\\\FallDetection\\\\Classes\\\\Falling\\\\Falling_479.npy'\n\n\n\t [[{{node PyFunc}}]] [Op:IteratorGetNextSync]"
     ]
    }
   ],
   "source": [
    "#tf 2.0\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(os.path.join('tmp'), update_freq=1000)\n",
    "#model.fit(train_dataset, epochs=1, callbacks=[tensorboard_callback], validation_data=valid_dataset)\n",
    "model.fit(train_dataset, epochs=1,validation_data=valid_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "This model has not yet been built. Build the model first by calling `build()` or calling `fit()` with some data, or specify an `input_shape` argument in the first layer(s) for automatic build.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-28-5f15418b3570>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\network.py\u001b[0m in \u001b[0;36msummary\u001b[1;34m(self, line_length, positions, print_fn)\u001b[0m\n\u001b[0;32m   1255\u001b[0m     \"\"\"\n\u001b[0;32m   1256\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuilt\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1257\u001b[1;33m       raise ValueError('This model has not yet been built. '\n\u001b[0m\u001b[0;32m   1258\u001b[0m                        \u001b[1;34m'Build the model first by calling `build()` or calling '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1259\u001b[0m                        \u001b[1;34m'`fit()` with some data, or specify '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: This model has not yet been built. Build the model first by calling `build()` or calling `fit()` with some data, or specify an `input_shape` argument in the first layer(s) for automatic build."
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnknownError",
     "evalue": "FileNotFoundError: [Errno 2] No such file or directory: 'C:\\\\Users\\\\STSC.LNVO-126908.000\\\\Desktop\\\\FallDetection\\\\Classes\\\\Falling\\\\Falling_477.npy'\nTraceback (most recent call last):\n\n  File \"C:\\Users\\STSC.LNVO-126908.000\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\script_ops.py\", line 221, in __call__\n    ret = func(*args)\n\n  File \"C:\\Users\\STSC.LNVO-126908.000\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\data\\ops\\dataset_ops.py\", line 585, in generator_py_func\n    values = next(generator_state.get_iterator(iterator_id))\n\n  File \"<ipython-input-14-b441973f18d5>\", line 20, in generator\n    features = np.load(full_path)\n\n  File \"C:\\Users\\STSC.LNVO-126908.000\\Anaconda3\\lib\\site-packages\\numpy\\lib\\npyio.py\", line 422, in load\n    fid = open(os_fspath(file), \"rb\")\n\nFileNotFoundError: [Errno 2] No such file or directory: 'C:\\\\Users\\\\STSC.LNVO-126908.000\\\\Desktop\\\\FallDetection\\\\Classes\\\\Falling\\\\Falling_477.npy'\n\n\n\t [[{{node PyFunc}}]] [Op:IteratorGetNextSync]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnknownError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-30-db263e23ff98>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#tf 1.1.4\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mvalid_dataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    727\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 728\u001b[1;33m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    729\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    730\u001b[0m   def evaluate(self,\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[0;32m    222\u001b[0m           \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    223\u001b[0m           \u001b[0mvalidation_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 224\u001b[1;33m           distribution_strategy=strategy)\n\u001b[0m\u001b[0;32m    225\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    226\u001b[0m       \u001b[0mtotal_samples\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_get_total_number_of_samples\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraining_data_adapter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36m_process_training_inputs\u001b[1;34m(model, x, y, batch_size, epochs, sample_weights, class_weights, steps_per_epoch, validation_split, validation_data, validation_steps, shuffle, distribution_strategy, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m    545\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    546\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 547\u001b[1;33m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    548\u001b[0m     \u001b[0mval_adapter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36m_process_inputs\u001b[1;34m(model, x, y, batch_size, epochs, sample_weights, class_weights, shuffle, steps, distribution_strategy, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m    592\u001b[0m         \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    593\u001b[0m         \u001b[0mcheck_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 594\u001b[1;33m         steps=steps)\n\u001b[0m\u001b[0;32m    595\u001b[0m   adapter = adapter_cls(\n\u001b[0;32m    596\u001b[0m       \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle, extract_tensors_from_dataset)\u001b[0m\n\u001b[0;32m   2417\u001b[0m     \u001b[1;31m# First, we build the model on the fly if necessary.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2418\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2419\u001b[1;33m       \u001b[0mall_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_input\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdict_inputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_build_model_with_inputs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2420\u001b[0m       \u001b[0mis_build_called\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2421\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_build_model_with_inputs\u001b[1;34m(self, inputs, targets)\u001b[0m\n\u001b[0;32m   2576\u001b[0m     \u001b[1;31m# tensors from the iterator and then standardize them.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2577\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mdataset_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDatasetV1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDatasetV2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2578\u001b[1;33m       \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtraining_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextract_tensors_from_dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2579\u001b[0m     \u001b[1;31m# We type-check that `inputs` and `targets` are either single arrays\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2580\u001b[0m     \u001b[1;31m# or lists of arrays, and extract a flat list of inputs from the passed\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_utils.py\u001b[0m in \u001b[0;36mextract_tensors_from_dataset\u001b[1;34m(dataset)\u001b[0m\n\u001b[0;32m   1580\u001b[0m   \"\"\"\n\u001b[0;32m   1581\u001b[0m   \u001b[0miterator\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_iterator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1582\u001b[1;33m   \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0munpack_iterator_input\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1583\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1584\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_utils.py\u001b[0m in \u001b[0;36munpack_iterator_input\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m   1594\u001b[0m   \"\"\"\n\u001b[0;32m   1595\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1596\u001b[1;33m     \u001b[0mnext_element\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_next\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1597\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1598\u001b[0m     raise RuntimeError('Your dataset iterator ran out of data; '\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\data\\ops\\iterator_ops.py\u001b[0m in \u001b[0;36mget_next\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m    735\u001b[0m     \"\"\"\n\u001b[0;32m    736\u001b[0m     \u001b[1;32mdel\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 737\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_internal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    738\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    739\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_gather_saveables_for_checkpoint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\data\\ops\\iterator_ops.py\u001b[0m in \u001b[0;36m_next_internal\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    649\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterator_resource\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    650\u001b[0m             \u001b[0moutput_types\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_flat_output_types\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 651\u001b[1;33m             output_shapes=self._flat_output_shapes)\n\u001b[0m\u001b[0;32m    652\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    653\u001b[0m       \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\gen_dataset_ops.py\u001b[0m in \u001b[0;36miterator_get_next_sync\u001b[1;34m(iterator, output_types, output_shapes, name)\u001b[0m\n\u001b[0;32m   2670\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2671\u001b[0m         \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2672\u001b[1;33m       \u001b[0m_six\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2673\u001b[0m   \u001b[1;31m# Add nodes to the TensorFlow graph.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2674\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_types\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\six.py\u001b[0m in \u001b[0;36mraise_from\u001b[1;34m(value, from_value)\u001b[0m\n",
      "\u001b[1;31mUnknownError\u001b[0m: FileNotFoundError: [Errno 2] No such file or directory: 'C:\\\\Users\\\\STSC.LNVO-126908.000\\\\Desktop\\\\FallDetection\\\\Classes\\\\Falling\\\\Falling_477.npy'\nTraceback (most recent call last):\n\n  File \"C:\\Users\\STSC.LNVO-126908.000\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\script_ops.py\", line 221, in __call__\n    ret = func(*args)\n\n  File \"C:\\Users\\STSC.LNVO-126908.000\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\data\\ops\\dataset_ops.py\", line 585, in generator_py_func\n    values = next(generator_state.get_iterator(iterator_id))\n\n  File \"<ipython-input-14-b441973f18d5>\", line 20, in generator\n    features = np.load(full_path)\n\n  File \"C:\\Users\\STSC.LNVO-126908.000\\Anaconda3\\lib\\site-packages\\numpy\\lib\\npyio.py\", line 422, in load\n    fid = open(os_fspath(file), \"rb\")\n\nFileNotFoundError: [Errno 2] No such file or directory: 'C:\\\\Users\\\\STSC.LNVO-126908.000\\\\Desktop\\\\FallDetection\\\\Classes\\\\Falling\\\\Falling_477.npy'\n\n\n\t [[{{node PyFunc}}]] [Op:IteratorGetNextSync]"
     ]
    }
   ],
   "source": [
    "#tf 1.1.4\n",
    "model.fit(train_dataset,epochs=1, validation_data= valid_dataset, validation_steps=4,verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:/Users/STSC.LNVO-126908.000/Desktop/FallDetection\\train_log\\train\n"
     ]
    },
    {
     "ename": "UnknownError",
     "evalue": "FileNotFoundError: [Errno 2] No such file or directory: 'C:\\\\Users\\\\STSC.LNVO-126908.000\\\\Desktop\\\\FallDetection\\\\Classes\\\\Walking\\\\Walking286.npy'\nTraceback (most recent call last):\n\n  File \"C:\\Users\\STSC.LNVO-126908.000\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\script_ops.py\", line 221, in __call__\n    ret = func(*args)\n\n  File \"C:\\Users\\STSC.LNVO-126908.000\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\data\\ops\\dataset_ops.py\", line 585, in generator_py_func\n    values = next(generator_state.get_iterator(iterator_id))\n\n  File \"<ipython-input-14-b441973f18d5>\", line 20, in generator\n    features = np.load(full_path)\n\n  File \"C:\\Users\\STSC.LNVO-126908.000\\Anaconda3\\lib\\site-packages\\numpy\\lib\\npyio.py\", line 422, in load\n    fid = open(os_fspath(file), \"rb\")\n\nFileNotFoundError: [Errno 2] No such file or directory: 'C:\\\\Users\\\\STSC.LNVO-126908.000\\\\Desktop\\\\FallDetection\\\\Classes\\\\Walking\\\\Walking286.npy'\n\n\n\t [[{{node PyFunc}}]] [Op:IteratorGetNextSync]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnknownError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-32-f95a89374d94>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m           \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m17\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m           \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalid_dataset\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m           callbacks=[tensorboard_callback])\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;31m#model.fit(train_dataset, epochs=17, callbacks=[tensorboard_callback], validation_data=valid_dataset, validation_steps=4, verbose=0)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    727\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 728\u001b[1;33m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    729\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    730\u001b[0m   def evaluate(self,\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[0;32m    222\u001b[0m           \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    223\u001b[0m           \u001b[0mvalidation_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 224\u001b[1;33m           distribution_strategy=strategy)\n\u001b[0m\u001b[0;32m    225\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    226\u001b[0m       \u001b[0mtotal_samples\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_get_total_number_of_samples\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraining_data_adapter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36m_process_training_inputs\u001b[1;34m(model, x, y, batch_size, epochs, sample_weights, class_weights, steps_per_epoch, validation_split, validation_data, validation_steps, shuffle, distribution_strategy, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m    545\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    546\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 547\u001b[1;33m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    548\u001b[0m     \u001b[0mval_adapter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36m_process_inputs\u001b[1;34m(model, x, y, batch_size, epochs, sample_weights, class_weights, shuffle, steps, distribution_strategy, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m    592\u001b[0m         \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    593\u001b[0m         \u001b[0mcheck_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 594\u001b[1;33m         steps=steps)\n\u001b[0m\u001b[0;32m    595\u001b[0m   adapter = adapter_cls(\n\u001b[0;32m    596\u001b[0m       \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle, extract_tensors_from_dataset)\u001b[0m\n\u001b[0;32m   2417\u001b[0m     \u001b[1;31m# First, we build the model on the fly if necessary.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2418\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2419\u001b[1;33m       \u001b[0mall_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_input\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdict_inputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_build_model_with_inputs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2420\u001b[0m       \u001b[0mis_build_called\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2421\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_build_model_with_inputs\u001b[1;34m(self, inputs, targets)\u001b[0m\n\u001b[0;32m   2576\u001b[0m     \u001b[1;31m# tensors from the iterator and then standardize them.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2577\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mdataset_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDatasetV1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDatasetV2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2578\u001b[1;33m       \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtraining_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextract_tensors_from_dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2579\u001b[0m     \u001b[1;31m# We type-check that `inputs` and `targets` are either single arrays\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2580\u001b[0m     \u001b[1;31m# or lists of arrays, and extract a flat list of inputs from the passed\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_utils.py\u001b[0m in \u001b[0;36mextract_tensors_from_dataset\u001b[1;34m(dataset)\u001b[0m\n\u001b[0;32m   1580\u001b[0m   \"\"\"\n\u001b[0;32m   1581\u001b[0m   \u001b[0miterator\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_iterator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1582\u001b[1;33m   \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0munpack_iterator_input\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1583\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1584\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_utils.py\u001b[0m in \u001b[0;36munpack_iterator_input\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m   1594\u001b[0m   \"\"\"\n\u001b[0;32m   1595\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1596\u001b[1;33m     \u001b[0mnext_element\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_next\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1597\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1598\u001b[0m     raise RuntimeError('Your dataset iterator ran out of data; '\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\data\\ops\\iterator_ops.py\u001b[0m in \u001b[0;36mget_next\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m    735\u001b[0m     \"\"\"\n\u001b[0;32m    736\u001b[0m     \u001b[1;32mdel\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 737\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_internal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    738\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    739\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_gather_saveables_for_checkpoint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\data\\ops\\iterator_ops.py\u001b[0m in \u001b[0;36m_next_internal\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    649\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterator_resource\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    650\u001b[0m             \u001b[0moutput_types\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_flat_output_types\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 651\u001b[1;33m             output_shapes=self._flat_output_shapes)\n\u001b[0m\u001b[0;32m    652\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    653\u001b[0m       \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\gen_dataset_ops.py\u001b[0m in \u001b[0;36miterator_get_next_sync\u001b[1;34m(iterator, output_types, output_shapes, name)\u001b[0m\n\u001b[0;32m   2670\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2671\u001b[0m         \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2672\u001b[1;33m       \u001b[0m_six\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2673\u001b[0m   \u001b[1;31m# Add nodes to the TensorFlow graph.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2674\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_types\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\six.py\u001b[0m in \u001b[0;36mraise_from\u001b[1;34m(value, from_value)\u001b[0m\n",
      "\u001b[1;31mUnknownError\u001b[0m: FileNotFoundError: [Errno 2] No such file or directory: 'C:\\\\Users\\\\STSC.LNVO-126908.000\\\\Desktop\\\\FallDetection\\\\Classes\\\\Walking\\\\Walking286.npy'\nTraceback (most recent call last):\n\n  File \"C:\\Users\\STSC.LNVO-126908.000\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\script_ops.py\", line 221, in __call__\n    ret = func(*args)\n\n  File \"C:\\Users\\STSC.LNVO-126908.000\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\data\\ops\\dataset_ops.py\", line 585, in generator_py_func\n    values = next(generator_state.get_iterator(iterator_id))\n\n  File \"<ipython-input-14-b441973f18d5>\", line 20, in generator\n    features = np.load(full_path)\n\n  File \"C:\\Users\\STSC.LNVO-126908.000\\Anaconda3\\lib\\site-packages\\numpy\\lib\\npyio.py\", line 422, in load\n    fid = open(os_fspath(file), \"rb\")\n\nFileNotFoundError: [Errno 2] No such file or directory: 'C:\\\\Users\\\\STSC.LNVO-126908.000\\\\Desktop\\\\FallDetection\\\\Classes\\\\Walking\\\\Walking286.npy'\n\n\n\t [[{{node PyFunc}}]] [Op:IteratorGetNextSync]"
     ]
    }
   ],
   "source": [
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=mylog_dir, update_freq=1000)\n",
    "print(os.path.join(mylog_dir, 'train'))\n",
    "#following call works for tensorflow 1.1.4\n",
    "model.fit(train_dataset, \n",
    "          epochs=17, \n",
    "          validation_data=valid_dataset, \n",
    "          callbacks=[tensorboard_callback])\n",
    "\n",
    "#model.fit(train_dataset, epochs=17, callbacks=[tensorboard_callback], validation_data=valid_dataset, validation_steps=4, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Weights for model sequential have not yet been created. Weights are created when the Model is first called on inputs or `build()` is called with an `input_shape`.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-33-20fd6e2cc0b1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBASE_PATH\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'my_model.h5'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\network.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(self, filepath, overwrite, include_optimizer, save_format, signatures, options)\u001b[0m\n\u001b[0;32m    973\u001b[0m     \"\"\"\n\u001b[0;32m    974\u001b[0m     saving.save_model(self, filepath, overwrite, include_optimizer, save_format,\n\u001b[1;32m--> 975\u001b[1;33m                       signatures, options)\n\u001b[0m\u001b[0;32m    976\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    977\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0msave_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilepath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msave_format\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\saving\\save.py\u001b[0m in \u001b[0;36msave_model\u001b[1;34m(model, filepath, overwrite, include_optimizer, save_format, signatures, options)\u001b[0m\n\u001b[0;32m    110\u001b[0m           'or using `save_weights`.')\n\u001b[0;32m    111\u001b[0m     hdf5_format.save_model_to_hdf5(\n\u001b[1;32m--> 112\u001b[1;33m         model, filepath, overwrite, include_optimizer)\n\u001b[0m\u001b[0;32m    113\u001b[0m   \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    114\u001b[0m     saved_model_save.save(model, filepath, overwrite, include_optimizer,\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\saving\\hdf5_format.py\u001b[0m in \u001b[0;36msave_model_to_hdf5\u001b[1;34m(model, filepath, overwrite, include_optimizer)\u001b[0m\n\u001b[0;32m     76\u001b[0m   \u001b[1;31m# entities like metrics added using `add_metric` and losses added using\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m   \u001b[1;31m# `add_loss.`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 78\u001b[1;33m   \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_undeduplicated_weights\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     79\u001b[0m     logging.warning('Found duplicated `Variable`s in Model\\'s `weights`. '\n\u001b[0;32m     80\u001b[0m                     \u001b[1;34m'This is usually caused by `Variable`s being shared by '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\network.py\u001b[0m in \u001b[0;36mweights\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    474\u001b[0m       \u001b[0mA\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mof\u001b[0m \u001b[0mvariables\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    475\u001b[0m     \"\"\"\n\u001b[1;32m--> 476\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dedup_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_undeduplicated_weights\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    477\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    478\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\network.py\u001b[0m in \u001b[0;36m_undeduplicated_weights\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    479\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_undeduplicated_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    480\u001b[0m     \u001b[1;34m\"\"\"Returns the undeduplicated list of all layer variables/weights.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 481\u001b[1;33m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_assert_weights_created\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    482\u001b[0m     \u001b[0mweights\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    483\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_layers\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\network.py\u001b[0m in \u001b[0;36m_assert_weights_created\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1421\u001b[0m                        \u001b[1;34m'Weights are created when the Model is first called on '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1422\u001b[0m                        \u001b[1;34m'inputs or `build()` is called with an `input_shape`.'\u001b[0m \u001b[1;33m%\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1423\u001b[1;33m                        self.name)\n\u001b[0m\u001b[0;32m   1424\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1425\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_graph_network_add_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msymbolic_loss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Weights for model sequential have not yet been created. Weights are created when the Model is first called on inputs or `build()` is called with an `input_shape`."
     ]
    }
   ],
   "source": [
    "model.file=os.path.join(BASE_PATH,'my_model.h5')\n",
    "model.save(model.file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model()\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "log_dir=\"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "model.fit(x=x_train, \n",
    "          y=y_train, \n",
    "          epochs=5, \n",
    "          validation_data=(x_test, y_test), \n",
    "          callbacks=[tensorboard_callback])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 12:  save the tensorflow model to an h5 file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DATA_PATH = '/Users/subhi/Downloads/CV-Proj3/Data'\n",
    "model_file = os.path.join(BASE_DATA_PATH, 'my_model.h5')\n",
    "# Save the entire model to a HDF5 file.\n",
    "# The '.h5' extension indicates that the model shuold be saved to HDF5.\n",
    "model.save(model_file) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 13: try to conver the model to tflite --- Support to come 2019 (when?)--Curently LSTM conversion to TFLite NOT supported"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " want to save tflite_file/Users/subhi/Downloads/CV-proj3/my_tflite_model.tflite\n"
     ]
    },
    {
     "ename": "ConverterError",
     "evalue": "See console for info.\n/Users/subhi/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n  from ._conv import register_converters as _register_converters\n2019-11-28 09:17:58.493144: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n2019-11-28 09:17:58.514648: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7f9f5d4d0410 executing computations on platform Host. Devices:\n2019-11-28 09:17:58.514665: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): Host, Default Version\n2019-11-28 09:17:58.569638: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorListFromTensor\n2019-11-28 09:17:58.569678: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 21\n2019-11-28 09:17:58.569697: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorListFromTensor\n2019-11-28 09:17:58.569704: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 21\n2019-11-28 09:17:58.569734: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorListReserve\n2019-11-28 09:17:58.569741: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 21\n2019-11-28 09:17:58.569841: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: While\n2019-11-28 09:17:58.569859: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 21\n2019-11-28 09:17:58.569863: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 21\n2019-11-28 09:17:58.569867: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 21\n2019-11-28 09:17:58.569889: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorListStack\n2019-11-28 09:17:58.574213: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 53 operators, 112 arrays (0 quantized)\n2019-11-28 09:17:58.574697: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 53 operators, 112 arrays (0 quantized)\n2019-11-28 09:17:58.610238: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 1: 30 operators, 85 arrays (0 quantized)\n2019-11-28 09:17:58.610722: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 2: 28 operators, 81 arrays (0 quantized)\n2019-11-28 09:17:58.611147: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Group bidirectional sequence lstm/rnn: 28 operators, 81 arrays (0 quantized)\n2019-11-28 09:17:58.611404: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before dequantization graph transformations: 28 operators, 81 arrays (0 quantized)\n2019-11-28 09:17:58.611930: I tensorflow/lite/toco/allocate_transient_arrays.cc:345] Total transient array allocated size: 460864 bytes, theoretical optimal value: 409600 bytes.\n2019-11-28 09:17:58.612467: E tensorflow/lite/toco/toco_tooling.cc:466] We are continually in the process of adding support to TensorFlow Lite for more ops. It would be helpful if you could inform us of how this conversion went by opening a github issue at https://github.com/tensorflow/tensorflow/issues/new?template=40-tflite-op-request.md\n and pasting the following:\n\nSome of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: ADD, CAST, FULLY_CONNECTED, LOGISTIC, MUL, NOT_EQUAL, REDUCE_ANY, RESHAPE, SOFTMAX, SPLIT, STRIDED_SLICE, TANH, ZEROS_LIKE. Here is a list of operators for which you will need custom implementations: TensorListFromTensor, TensorListReserve, TensorListStack, While.\nTraceback (most recent call last):\n  File \"/Users/subhi/anaconda3/bin/toco_from_protos\", line 8, in <module>\n    sys.exit(main())\n  File \"/Users/subhi/anaconda3/lib/python3.6/site-packages/tensorflow_core/lite/toco/python/toco_from_protos.py\", line 89, in main\n    app.run(main=execute, argv=[sys.argv[0]] + unparsed)\n  File \"/Users/subhi/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/platform/app.py\", line 40, in run\n    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\n  File \"/Users/subhi/anaconda3/lib/python3.6/site-packages/absl/app.py\", line 300, in run\n    _run_main(main, args)\n  File \"/Users/subhi/anaconda3/lib/python3.6/site-packages/absl/app.py\", line 251, in _run_main\n    sys.exit(main(argv))\n  File \"/Users/subhi/anaconda3/lib/python3.6/site-packages/tensorflow_core/lite/toco/python/toco_from_protos.py\", line 52, in execute\n    enable_mlir_converter)\nException: We are continually in the process of adding support to TensorFlow Lite for more ops. It would be helpful if you could inform us of how this conversion went by opening a github issue at https://github.com/tensorflow/tensorflow/issues/new?template=40-tflite-op-request.md\n and pasting the following:\n\nSome of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: ADD, CAST, FULLY_CONNECTED, LOGISTIC, MUL, NOT_EQUAL, REDUCE_ANY, RESHAPE, SOFTMAX, SPLIT, STRIDED_SLICE, TANH, ZEROS_LIKE. Here is a list of operators for which you will need custom implementations: TensorListFromTensor, TensorListReserve, TensorListStack, While.\n\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mConverterError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-166aadbf2b95>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Convert the model.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mconverter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTFLiteConverter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_keras_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mtflite_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconverter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow_core/lite/python/lite.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    444\u001b[0m         \u001b[0minput_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_tensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m         \u001b[0moutput_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_tensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 446\u001b[0;31m         **converter_kwargs)\n\u001b[0m\u001b[1;32m    447\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_calibration_quantize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow_core/lite/python/convert.py\u001b[0m in \u001b[0;36mtoco_convert_impl\u001b[0;34m(input_data, input_tensors, output_tensors, enable_mlir_converter, *args, **kwargs)\u001b[0m\n\u001b[1;32m    447\u001b[0m       \u001b[0minput_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSerializeToString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m       \u001b[0mdebug_info_str\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdebug_info_str\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 449\u001b[0;31m       enable_mlir_converter=enable_mlir_converter)\n\u001b[0m\u001b[1;32m    450\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow_core/lite/python/convert.py\u001b[0m in \u001b[0;36mtoco_convert_protos\u001b[0;34m(model_flags_str, toco_flags_str, input_data_str, debug_info_str, enable_mlir_converter)\u001b[0m\n\u001b[1;32m    198\u001b[0m       \u001b[0mstdout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_try_convert_to_unicode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m       \u001b[0mstderr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_try_convert_to_unicode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstderr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mConverterError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"See console for info.\\n%s\\n%s\\n\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstderr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    201\u001b[0m   \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m     \u001b[0;31m# Must manually cleanup files.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mConverterError\u001b[0m: See console for info.\n/Users/subhi/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n  from ._conv import register_converters as _register_converters\n2019-11-28 09:17:58.493144: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n2019-11-28 09:17:58.514648: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7f9f5d4d0410 executing computations on platform Host. Devices:\n2019-11-28 09:17:58.514665: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): Host, Default Version\n2019-11-28 09:17:58.569638: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorListFromTensor\n2019-11-28 09:17:58.569678: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 21\n2019-11-28 09:17:58.569697: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorListFromTensor\n2019-11-28 09:17:58.569704: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 21\n2019-11-28 09:17:58.569734: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorListReserve\n2019-11-28 09:17:58.569741: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 21\n2019-11-28 09:17:58.569841: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: While\n2019-11-28 09:17:58.569859: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 21\n2019-11-28 09:17:58.569863: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 21\n2019-11-28 09:17:58.569867: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 21\n2019-11-28 09:17:58.569889: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorListStack\n2019-11-28 09:17:58.574213: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 53 operators, 112 arrays (0 quantized)\n2019-11-28 09:17:58.574697: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 53 operators, 112 arrays (0 quantized)\n2019-11-28 09:17:58.610238: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 1: 30 operators, 85 arrays (0 quantized)\n2019-11-28 09:17:58.610722: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 2: 28 operators, 81 arrays (0 quantized)\n2019-11-28 09:17:58.611147: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Group bidirectional sequence lstm/rnn: 28 operators, 81 arrays (0 quantized)\n2019-11-28 09:17:58.611404: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before dequantization graph transformations: 28 operators, 81 arrays (0 quantized)\n2019-11-28 09:17:58.611930: I tensorflow/lite/toco/allocate_transient_arrays.cc:345] Total transient array allocated size: 460864 bytes, theoretical optimal value: 409600 bytes.\n2019-11-28 09:17:58.612467: E tensorflow/lite/toco/toco_tooling.cc:466] We are continually in the process of adding support to TensorFlow Lite for more ops. It would be helpful if you could inform us of how this conversion went by opening a github issue at https://github.com/tensorflow/tensorflow/issues/new?template=40-tflite-op-request.md\n and pasting the following:\n\nSome of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: ADD, CAST, FULLY_CONNECTED, LOGISTIC, MUL, NOT_EQUAL, REDUCE_ANY, RESHAPE, SOFTMAX, SPLIT, STRIDED_SLICE, TANH, ZEROS_LIKE. Here is a list of operators for which you will need custom implementations: TensorListFromTensor, TensorListReserve, TensorListStack, While.\nTraceback (most recent call last):\n  File \"/Users/subhi/anaconda3/bin/toco_from_protos\", line 8, in <module>\n    sys.exit(main())\n  File \"/Users/subhi/anaconda3/lib/python3.6/site-packages/tensorflow_core/lite/toco/python/toco_from_protos.py\", line 89, in main\n    app.run(main=execute, argv=[sys.argv[0]] + unparsed)\n  File \"/Users/subhi/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/platform/app.py\", line 40, in run\n    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\n  File \"/Users/subhi/anaconda3/lib/python3.6/site-packages/absl/app.py\", line 300, in run\n    _run_main(main, args)\n  File \"/Users/subhi/anaconda3/lib/python3.6/site-packages/absl/app.py\", line 251, in _run_main\n    sys.exit(main(argv))\n  File \"/Users/subhi/anaconda3/lib/python3.6/site-packages/tensorflow_core/lite/toco/python/toco_from_protos.py\", line 52, in execute\n    enable_mlir_converter)\nException: We are continually in the process of adding support to TensorFlow Lite for more ops. It would be helpful if you could inform us of how this conversion went by opening a github issue at https://github.com/tensorflow/tensorflow/issues/new?template=40-tflite-op-request.md\n and pasting the following:\n\nSome of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: ADD, CAST, FULLY_CONNECTED, LOGISTIC, MUL, NOT_EQUAL, REDUCE_ANY, RESHAPE, SOFTMAX, SPLIT, STRIDED_SLICE, TANH, ZEROS_LIKE. Here is a list of operators for which you will need custom implementations: TensorListFromTensor, TensorListReserve, TensorListStack, While.\n\n\n"
     ]
    }
   ],
   "source": [
    "#from tensorflow import lite\n",
    "tflite_file  = os.path.join(BASE_DATA_PATH, 'my_tflite_model.tflite')\n",
    "print(\" want to save tflite_file\" + tflite_file)\n",
    "# Convert the model.\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#now save the tflite model to the file\n",
    "#tflite_model.save(tflite_file)   #Note this does not seem to work although in google documentation\n",
    "open(tflite_file, \"wb\").write(tflite_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 14: run evaluation on the test data feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "# Evaluate on test data\n",
      "19/19 [==============================] - 3s 139ms/step - loss: 0.0429 - accuracy: 0.9934 - top_k_categorical_accuracy: 1.0000\n",
      "test loss, test acc: [0.042945761266672, 0.9934211, 1.0]\n"
     ]
    }
   ],
   "source": [
    "# evaluate the test data using model\n",
    "\n",
    "\n",
    "# Evaluate the model on the test data using `evaluate`\n",
    "print('\\n# Evaluate on test data')\n",
    "\n",
    "# NOTE: should have separate test data but, only have validation data\n",
    "#results = model.evaluate_generator(val_data_gen, verbose=1)\n",
    "results = model.evaluate(valid_dataset, verbose=1)\n",
    "print('test loss, test acc:', results)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 15: Run predictions on the test data feature extracted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "# Generate predictions \n",
      "19/19 [==============================] - 3s 168ms/step\n"
     ]
    }
   ],
   "source": [
    "# make predictions\n",
    "\n",
    "# Generate predictions (probabilities -- the output of the last layer)\n",
    "# on new data using `predict`\n",
    "print('\\n# Generate predictions ')\n",
    "predictions = model.predict(valid_dataset, verbose=1 )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictions shape: (304, 2)\n",
      "[[9.99998927e-01 1.03256002e-06]\n",
      " [1.67706972e-07 9.99999881e-01]\n",
      " [1.00000000e+00 6.82283674e-10]\n",
      " [1.11434516e-11 1.00000000e+00]\n",
      " [9.99999762e-01 2.29326602e-07]\n",
      " [9.99998808e-01 1.16833326e-06]\n",
      " [9.99999762e-01 1.82261132e-07]\n",
      " [9.99918461e-01 8.15016028e-05]\n",
      " [9.99926448e-01 7.35764625e-05]\n",
      " [9.99999881e-01 8.47332089e-08]\n",
      " [1.00000000e+00 3.23047611e-09]\n",
      " [9.99956131e-01 4.38320676e-05]\n",
      " [3.47217033e-03 9.96527851e-01]\n",
      " [9.99999285e-01 6.98495455e-07]\n",
      " [9.99999642e-01 3.91575156e-07]\n",
      " [7.44205808e-10 1.00000000e+00]\n",
      " [9.99999404e-01 5.40846315e-07]\n",
      " [1.52434044e-12 1.00000000e+00]\n",
      " [9.99999762e-01 2.48983440e-07]\n",
      " [3.14490760e-11 1.00000000e+00]\n",
      " [9.33684468e-01 6.63155392e-02]\n",
      " [6.11079451e-11 1.00000000e+00]\n",
      " [1.48474621e-09 1.00000000e+00]\n",
      " [9.99998212e-01 1.83439920e-06]\n",
      " [9.99998689e-01 1.30680746e-06]\n",
      " [6.44362656e-13 1.00000000e+00]\n",
      " [9.99998808e-01 1.13260558e-06]\n",
      " [9.99999642e-01 3.91971213e-07]\n",
      " [1.00000000e+00 3.20483657e-08]\n",
      " [1.94535144e-09 1.00000000e+00]\n",
      " [9.99999762e-01 1.95982352e-07]\n",
      " [9.99989867e-01 1.00865918e-05]\n",
      " [9.99999881e-01 6.05541288e-08]\n",
      " [9.99999881e-01 1.11010237e-07]\n",
      " [9.99997616e-01 2.42166902e-06]\n",
      " [9.99999285e-01 7.59721672e-07]\n",
      " [9.99998808e-01 1.21196740e-06]\n",
      " [9.99999523e-01 5.33562400e-07]\n",
      " [9.99999523e-01 5.30562602e-07]\n",
      " [9.99999881e-01 1.74695941e-07]\n",
      " [9.99602020e-01 3.97987867e-04]\n",
      " [1.06445204e-05 9.99989390e-01]\n",
      " [9.99999762e-01 2.48502118e-07]\n",
      " [9.99999523e-01 4.37604996e-07]\n",
      " [9.99990106e-01 9.87811563e-06]\n",
      " [9.99999166e-01 7.96240613e-07]\n",
      " [9.99999881e-01 9.55170023e-08]\n",
      " [9.99999762e-01 2.96961531e-07]\n",
      " [9.99999881e-01 1.07294582e-07]\n",
      " [9.99998927e-01 1.02965134e-06]\n",
      " [9.99999762e-01 1.83593428e-07]\n",
      " [1.28645979e-07 9.99999881e-01]\n",
      " [9.99999762e-01 2.14034131e-07]\n",
      " [9.99999404e-01 5.62567152e-07]\n",
      " [9.99996305e-01 3.65244136e-06]\n",
      " [9.99996781e-01 3.18980028e-06]\n",
      " [9.99995589e-01 4.45494607e-06]\n",
      " [9.99997973e-01 2.07709240e-06]\n",
      " [1.00000000e+00 3.68431472e-08]\n",
      " [4.80035976e-12 1.00000000e+00]\n",
      " [9.99999881e-01 9.02968296e-08]\n",
      " [9.99985576e-01 1.43728921e-05]\n",
      " [9.99999642e-01 3.29877736e-07]\n",
      " [1.00000000e+00 5.14791374e-08]\n",
      " [9.99993563e-01 6.44496049e-06]\n",
      " [5.42702310e-08 1.00000000e+00]\n",
      " [9.99999523e-01 5.02865305e-07]\n",
      " [2.24688912e-09 1.00000000e+00]\n",
      " [1.00000000e+00 2.86329915e-09]\n",
      " [8.44913306e-11 1.00000000e+00]\n",
      " [9.99999881e-01 6.28305230e-08]\n",
      " [3.04916284e-07 9.99999642e-01]\n",
      " [1.92989668e-04 9.99807060e-01]\n",
      " [9.99999762e-01 2.64889877e-07]\n",
      " [9.99997377e-01 2.59733997e-06]\n",
      " [1.98620829e-12 1.00000000e+00]\n",
      " [9.99998450e-01 1.50383039e-06]\n",
      " [9.99999642e-01 3.60147794e-07]\n",
      " [6.49278742e-10 1.00000000e+00]\n",
      " [1.00000000e+00 4.21667945e-09]\n",
      " [9.99999642e-01 3.11065179e-07]\n",
      " [9.97386038e-01 2.61398358e-03]\n",
      " [9.99999046e-01 9.33584431e-07]\n",
      " [9.99995470e-01 4.49643312e-06]\n",
      " [9.99995708e-01 4.27693203e-06]\n",
      " [1.00000000e+00 2.44131160e-09]\n",
      " [3.02035397e-10 1.00000000e+00]\n",
      " [2.02524567e-10 1.00000000e+00]\n",
      " [9.99992371e-01 7.66253925e-06]\n",
      " [4.86476311e-11 1.00000000e+00]\n",
      " [9.99999285e-01 7.13579482e-07]\n",
      " [9.99999762e-01 2.28615178e-07]\n",
      " [9.99999404e-01 5.47349032e-07]\n",
      " [1.49315092e-11 1.00000000e+00]\n",
      " [9.99999046e-01 9.32843079e-07]\n",
      " [1.15237153e-05 9.99988437e-01]\n",
      " [9.99935865e-01 6.41077932e-05]\n",
      " [9.99988914e-01 1.11408117e-05]\n",
      " [9.99999523e-01 5.14571980e-07]\n",
      " [9.99971271e-01 2.87094972e-05]\n",
      " [9.99990940e-01 9.08849688e-06]\n",
      " [3.13586462e-10 1.00000000e+00]\n",
      " [9.99999762e-01 2.15344500e-07]\n",
      " [9.99999523e-01 4.26335674e-07]\n",
      " [9.99999881e-01 1.30525393e-07]\n",
      " [1.40053213e-09 1.00000000e+00]\n",
      " [9.99999881e-01 9.09325095e-08]\n",
      " [9.99999881e-01 7.59614522e-08]\n",
      " [9.99999881e-01 1.72130285e-07]\n",
      " [9.99996424e-01 3.59261776e-06]\n",
      " [9.99997973e-01 2.05905235e-06]\n",
      " [1.47131323e-06 9.99998569e-01]\n",
      " [1.00000000e+00 2.42505109e-08]\n",
      " [9.99990463e-01 9.53991366e-06]\n",
      " [1.00000000e+00 3.40811006e-08]\n",
      " [9.99998689e-01 1.31969216e-06]\n",
      " [9.99999881e-01 1.25178602e-07]\n",
      " [5.80376316e-08 1.00000000e+00]\n",
      " [9.99998331e-01 1.71409977e-06]\n",
      " [9.99998808e-01 1.24609915e-06]\n",
      " [9.99999762e-01 2.79592768e-07]\n",
      " [9.99998569e-01 1.45293279e-06]\n",
      " [9.99997735e-01 2.20703760e-06]\n",
      " [2.86445334e-06 9.99997139e-01]\n",
      " [2.51168170e-10 1.00000000e+00]\n",
      " [9.99958158e-01 4.18310410e-05]\n",
      " [9.99962568e-01 3.74381343e-05]\n",
      " [9.24485288e-09 1.00000000e+00]\n",
      " [4.46184201e-11 1.00000000e+00]\n",
      " [1.00000000e+00 2.92076550e-08]\n",
      " [9.99999046e-01 9.97410211e-07]\n",
      " [9.99991179e-01 8.83962548e-06]\n",
      " [9.99999523e-01 4.19267877e-07]\n",
      " [9.99999285e-01 7.65363382e-07]\n",
      " [9.99999523e-01 5.23490939e-07]\n",
      " [5.80537964e-08 1.00000000e+00]\n",
      " [6.66381316e-12 1.00000000e+00]\n",
      " [1.00000000e+00 3.79468972e-08]\n",
      " [9.99999881e-01 7.05512306e-08]\n",
      " [1.00000000e+00 5.89880322e-08]\n",
      " [1.52673512e-12 1.00000000e+00]\n",
      " [1.52932907e-12 1.00000000e+00]\n",
      " [9.99993563e-01 6.40498547e-06]\n",
      " [9.99998450e-01 1.54973782e-06]\n",
      " [9.99999404e-01 5.56240025e-07]\n",
      " [1.00000000e+00 3.74548321e-08]\n",
      " [9.99992847e-01 7.15782153e-06]\n",
      " [1.32934520e-06 9.99998689e-01]\n",
      " [2.42853370e-02 9.75714624e-01]\n",
      " [9.99993205e-01 6.84536826e-06]\n",
      " [1.58438401e-11 1.00000000e+00]\n",
      " [9.99998212e-01 1.73192643e-06]\n",
      " [9.99999166e-01 8.88506122e-07]\n",
      " [9.99996305e-01 3.65975643e-06]\n",
      " [1.00000000e+00 5.19555776e-08]\n",
      " [9.99999642e-01 3.10081248e-07]\n",
      " [1.13692100e-09 1.00000000e+00]\n",
      " [7.46816553e-10 1.00000000e+00]\n",
      " [1.16682011e-10 1.00000000e+00]\n",
      " [9.99999881e-01 1.34269257e-07]\n",
      " [9.99900460e-01 9.94823058e-05]\n",
      " [1.77455632e-07 9.99999881e-01]\n",
      " [9.99999881e-01 8.39443359e-08]\n",
      " [9.99995232e-01 4.80872768e-06]\n",
      " [9.99999762e-01 2.10553537e-07]\n",
      " [1.17362075e-01 8.82637978e-01]\n",
      " [9.84477758e-01 1.55222323e-02]\n",
      " [1.58652735e-08 1.00000000e+00]\n",
      " [1.00000000e+00 2.62820254e-09]\n",
      " [9.99999762e-01 2.72974347e-07]\n",
      " [9.99999881e-01 8.80009381e-08]\n",
      " [6.39787078e-01 3.60212922e-01]\n",
      " [1.00000000e+00 1.19496280e-09]\n",
      " [9.95166421e-01 4.83356928e-03]\n",
      " [9.99999881e-01 8.91101308e-08]\n",
      " [9.99991417e-01 8.58804742e-06]\n",
      " [9.99993443e-01 6.56572229e-06]\n",
      " [9.99999762e-01 2.00183678e-07]\n",
      " [2.87169550e-12 1.00000000e+00]\n",
      " [2.56537803e-12 1.00000000e+00]\n",
      " [1.39609620e-05 9.99986053e-01]\n",
      " [6.10795770e-09 1.00000000e+00]\n",
      " [9.99999881e-01 1.46266856e-07]\n",
      " [9.99999523e-01 4.63522383e-07]\n",
      " [9.99999404e-01 5.88585578e-07]\n",
      " [9.99999881e-01 1.41645074e-07]\n",
      " [9.99997854e-01 2.16143326e-06]\n",
      " [8.71245920e-09 1.00000000e+00]\n",
      " [9.99999166e-01 7.81465758e-07]\n",
      " [9.99999881e-01 1.55706786e-07]\n",
      " [9.99993563e-01 6.37836047e-06]\n",
      " [9.99995708e-01 4.23814026e-06]\n",
      " [1.00000000e+00 6.26945074e-09]\n",
      " [9.99999523e-01 4.33783157e-07]\n",
      " [9.99999285e-01 6.67298366e-07]\n",
      " [9.99999166e-01 7.77926289e-07]\n",
      " [9.99905109e-01 9.48876550e-05]\n",
      " [9.99999881e-01 1.45349375e-07]\n",
      " [6.39516002e-05 9.99936104e-01]\n",
      " [9.99999166e-01 8.18204740e-07]\n",
      " [1.32268849e-06 9.99998689e-01]\n",
      " [1.00000000e+00 5.01442976e-08]\n",
      " [9.99997616e-01 2.40495092e-06]\n",
      " [9.99998808e-01 1.14058116e-06]\n",
      " [2.08670869e-07 9.99999762e-01]\n",
      " [9.99999404e-01 5.60129195e-07]\n",
      " [1.90362152e-05 9.99980927e-01]\n",
      " [9.99998569e-01 1.46958291e-06]\n",
      " [9.99999881e-01 1.75450609e-07]\n",
      " [1.62142000e-09 1.00000000e+00]\n",
      " [1.00000000e+00 4.98116108e-08]\n",
      " [9.99995232e-01 4.81637335e-06]\n",
      " [1.00000000e+00 3.80903309e-08]\n",
      " [6.51771714e-13 1.00000000e+00]\n",
      " [8.36616609e-10 1.00000000e+00]\n",
      " [4.74548436e-08 1.00000000e+00]\n",
      " [1.00000000e+00 3.52445326e-08]\n",
      " [9.99993324e-01 6.69501469e-06]\n",
      " [9.99999642e-01 3.14468195e-07]\n",
      " [9.99999404e-01 5.85067141e-07]\n",
      " [9.99980569e-01 1.94037239e-05]\n",
      " [9.99999881e-01 1.50416170e-07]\n",
      " [2.76353859e-07 9.99999762e-01]\n",
      " [9.99998569e-01 1.45437730e-06]\n",
      " [9.99999642e-01 4.02702597e-07]\n",
      " [1.72428634e-11 1.00000000e+00]\n",
      " [1.00000000e+00 1.24609691e-08]\n",
      " [9.99999881e-01 1.30055213e-07]\n",
      " [7.28794606e-04 9.99271214e-01]\n",
      " [9.99994397e-01 5.63804815e-06]\n",
      " [2.18950994e-02 9.78104889e-01]\n",
      " [9.99997735e-01 2.27214196e-06]\n",
      " [9.99997616e-01 2.41544126e-06]\n",
      " [9.99984980e-01 1.49973048e-05]\n",
      " [1.00000000e+00 5.17125684e-08]\n",
      " [3.21462093e-11 1.00000000e+00]\n",
      " [6.13381401e-10 1.00000000e+00]\n",
      " [1.00000000e+00 4.90752861e-08]\n",
      " [9.99999881e-01 1.46349450e-07]\n",
      " [9.99999285e-01 7.43217754e-07]\n",
      " [9.99998450e-01 1.60670288e-06]\n",
      " [1.00000000e+00 3.58345993e-08]\n",
      " [9.99999166e-01 8.74166517e-07]\n",
      " [9.99929190e-01 7.08225343e-05]\n",
      " [5.16451450e-08 1.00000000e+00]\n",
      " [9.99999642e-01 3.52653160e-07]\n",
      " [9.99999881e-01 7.93732795e-08]\n",
      " [9.99995232e-01 4.76420337e-06]\n",
      " [9.99979019e-01 2.09730533e-05]\n",
      " [9.99994040e-01 6.00572230e-06]\n",
      " [6.13381401e-10 1.00000000e+00]\n",
      " [1.93749372e-09 1.00000000e+00]\n",
      " [9.99999881e-01 7.34868735e-08]\n",
      " [9.99999642e-01 2.98684938e-07]\n",
      " [9.99999881e-01 1.09040784e-07]\n",
      " [9.66113003e-04 9.99033928e-01]\n",
      " [9.99998450e-01 1.51358836e-06]\n",
      " [9.99994874e-01 5.15899956e-06]\n",
      " [9.61867154e-01 3.81328352e-02]\n",
      " [9.99996781e-01 3.16680962e-06]\n",
      " [5.98048625e-11 1.00000000e+00]\n",
      " [9.99999285e-01 7.36200093e-07]\n",
      " [8.57698069e-06 9.99991417e-01]\n",
      " [9.99994278e-01 5.71554483e-06]\n",
      " [9.99996543e-01 3.47302102e-06]\n",
      " [9.99999881e-01 8.31542479e-08]\n",
      " [9.99994755e-01 5.20519689e-06]\n",
      " [1.49101717e-10 1.00000000e+00]\n",
      " [9.99998093e-01 1.95906455e-06]\n",
      " [3.43115571e-05 9.99965668e-01]\n",
      " [2.10667622e-07 9.99999762e-01]\n",
      " [1.00000000e+00 2.04808135e-08]\n",
      " [2.28319325e-11 1.00000000e+00]\n",
      " [5.77026871e-10 1.00000000e+00]\n",
      " [9.99996901e-01 3.05923982e-06]\n",
      " [9.99998569e-01 1.43720297e-06]\n",
      " [9.99991775e-01 8.16699958e-06]\n",
      " [9.99997139e-01 2.85996589e-06]\n",
      " [9.99938130e-01 6.19092389e-05]\n",
      " [9.99999881e-01 6.31336121e-08]\n",
      " [5.97754661e-06 9.99994040e-01]\n",
      " [4.18320815e-06 9.99995828e-01]\n",
      " [5.34028594e-12 1.00000000e+00]\n",
      " [9.99999404e-01 5.63840103e-07]\n",
      " [9.99912977e-01 8.69704236e-05]\n",
      " [9.99999881e-01 1.45866778e-07]\n",
      " [2.00831643e-10 1.00000000e+00]\n",
      " [9.99999881e-01 1.69389267e-07]\n",
      " [9.99999762e-01 1.91926318e-07]\n",
      " [1.68917502e-08 1.00000000e+00]\n",
      " [9.99998927e-01 1.05517120e-06]\n",
      " [9.99999285e-01 7.26656367e-07]\n",
      " [9.99999881e-01 1.77077624e-07]\n",
      " [9.99999881e-01 8.09336882e-08]\n",
      " [9.99988317e-01 1.17293785e-05]\n",
      " [9.99999881e-01 8.01529723e-08]\n",
      " [9.99998927e-01 1.03130628e-06]\n",
      " [2.31301783e-10 1.00000000e+00]\n",
      " [9.99997377e-01 2.63891411e-06]\n",
      " [9.99995470e-01 4.58302793e-06]\n",
      " [9.99999285e-01 6.76273203e-07]\n",
      " [4.84227520e-08 1.00000000e+00]\n",
      " [9.99999881e-01 1.48354161e-07]\n",
      " [9.99999523e-01 5.28893963e-07]]\n",
      "304\n"
     ]
    }
   ],
   "source": [
    "#print out prediction info for validation data set (as do not have separate test data set)\n",
    "print('predictions shape:', predictions.shape)\n",
    "print(predictions)\n",
    "print(len(predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
